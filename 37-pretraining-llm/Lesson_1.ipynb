{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Install dependencies and fix seed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad4b1310e8cae43d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T12:46:03.411869Z",
     "start_time": "2024-07-29T12:46:03.397997Z"
    }
   },
   "id": "d0f3a623780fbbc9",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-29T12:46:03.899444Z",
     "start_time": "2024-07-29T12:46:03.890155Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def fic_torch_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "fic_torch_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Load a general pretrained model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f547134889310ec7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_path_or_name = \"./models/upstage/TinySolar-248m-4k\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T12:46:04.746734Z",
     "start_time": "2024-07-29T12:46:04.743482Z"
    }
   },
   "id": "bcd360b50c465f3a",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "\n",
    "tiny_general_model = AutoModelForCausalLM.from_pretrained(model_path_or_name,\n",
    "                                                          device_map=\"cpu\",\n",
    "                                                          torch_dtype=torch.bfloat16)\n",
    "\n",
    "tiny_general_tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T12:46:05.704105Z",
     "start_time": "2024-07-29T12:46:05.229973Z"
    }
   },
   "id": "12c9cef94f29574d",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate text samples"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7da2cbe150155bf1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt = \"I am an engineer. i love\"\n",
    "\n",
    "inputs = tiny_general_tokenizer(prompt, return_tensors=\"pt\")\n",
    "streamer = TextStreamer(tiny_general_tokenizer, skip_prompt=True, skip_special_tokens=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T12:46:06.555433Z",
     "start_time": "2024-07-29T12:46:06.539735Z"
    }
   },
   "id": "9d9b12c2f8380d9e",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to read and write, but i'm not sure if it is the right time for me to start writing again.\n",
      "I have a lot of things in my head that I want to do, but I don't know how to get started.\n",
      "I've been working on this project for about 10 years now. I've been doing it for over 20 years. I've been doing it for about 30 years. I've been doing it for about 5 years now. I've been doing it for about 40 years. I've been doing it for about\n"
     ]
    }
   ],
   "source": [
    "outputs = tiny_general_model.generate(\n",
    "    **inputs,\n",
    "    streamer=streamer,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    repetition_penalty=1.1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T12:46:09.612428Z",
     "start_time": "2024-07-29T12:46:07.097178Z"
    }
   },
   "id": "b4122a1163639109",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Generate Python samples with pretrained general model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "838d3c5495258b6e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt = \"def find_max(numbers):\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T12:47:56.546648Z",
     "start_time": "2024-07-29T12:47:56.533022Z"
    }
   },
   "id": "466ec2583265684a",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "inputs = tiny_general_tokenizer(prompt, return_tensors=\"pt\").to(tiny_general_model.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tiny_general_tokenizer,\n",
    "    skip_prompt=True, skip_special_tokens=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T12:48:55.003135Z",
     "start_time": "2024-07-29T12:48:54.983989Z"
    }
   },
   "id": "4ce7ef38ddb34b7b",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       \"\"\"\n",
      "       Returns the number of times a user has been added to the list.\n",
      "       \"\"\"\n",
      "       return num_users() + 1\n",
      "   def get_user_id(self, id):\n",
      "       \"\"\"\n",
      "       Returns the number of users that have been added to the list.\n",
      "       \"\"\"\n",
      "       return len(self.get_users())\n",
      "   def get_user_name(self, name):\n",
      "       \"\"\"\n",
      "       Returns the name of the user that has been added to the list.\n",
      "       \"\"\"\n",
      "       return self.get_user_name(name)\n"
     ]
    }
   ],
   "source": [
    "outputs = tiny_general_model.generate(\n",
    "    **inputs, streamer=streamer, use_cache=True, max_new_tokens=128, do_sample=False, temperature=0.0,\n",
    "    repetition_penalty=1.1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T12:49:51.122343Z",
     "start_time": "2024-07-29T12:49:48.068324Z"
    }
   },
   "id": "2482a3fc3c0b18da",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Generate Python samples with finetuned Python model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9201915ee55f7bd9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_path_or_name = \"./models/upstage/TinySolar-248m-4k-code-instruct\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T12:54:35.285456Z",
     "start_time": "2024-07-29T12:54:35.260658Z"
    }
   },
   "id": "850f8b03653bca53",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tiny_finetuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path_or_name,\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tiny_finetuned_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path_or_name\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T12:54:36.651841Z",
     "start_time": "2024-07-29T12:54:36.009919Z"
    }
   },
   "id": "2d774d61e870cddf",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   if len(numbers) == 0:\n",
      "       return \"Invalid input\"\n",
      "   else:\n",
      "       return max(numbers)\n",
      "```\n",
      "In this solution, the `find_max` function takes a list of numbers as input and returns the maximum value in that list. It then iterates through each number in the list and checks if it is greater than or equal to 1. If it is, it adds it to the `max` list. Finally, it returns the maximum value found so far.\n"
     ]
    }
   ],
   "source": [
    "prompt =  \"def find_max(numbers):\"\n",
    "\n",
    "inputs = tiny_finetuned_tokenizer(\n",
    "    prompt, return_tensors=\"pt\"\n",
    ").to(tiny_finetuned_model.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tiny_finetuned_tokenizer,\n",
    "    skip_prompt=True,\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "outputs = tiny_finetuned_model.generate(\n",
    "    **inputs,\n",
    "    streamer=streamer,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    repetition_penalty=1.1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T12:54:40.284687Z",
     "start_time": "2024-07-29T12:54:37.171156Z"
    }
   },
   "id": "9d8a57bf16a5cd2d",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Generate Python samples with pretrained Python model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31aacfdd0aea96bb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_path_or_name = \"./models/upstage/TinySolar-248m-4k-py\" "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T13:08:20.937762Z",
     "start_time": "2024-07-29T13:08:20.917823Z"
    }
   },
   "id": "986606e658810267",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tiny_custom_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path_or_name,\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=torch.bfloat16,    \n",
    ")\n",
    "\n",
    "tiny_custom_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path_or_name\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T13:08:21.985027Z",
     "start_time": "2024-07-29T13:08:21.556535Z"
    }
   },
   "id": "3bd695ead4648a49",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \"\"\"Find the maximum number of numbers in a list.\"\"\"\n",
      "   max = 0\n",
      "   for num in numbers:\n",
      "       if num > max:\n",
      "           max = num\n",
      "   return max\n",
      "def get_min_max(numbers, min_value=1):\n",
      "   \"\"\"Get the minimum value of a list.\"\"\"\n",
      "   min_value = min_value or 1\n",
      "   for num in numbers:\n",
      "       if num < min_value:\n",
      "           min_value = num\n",
      "   return min_value\n"
     ]
    }
   ],
   "source": [
    "prompt = \"def find_max(numbers):\"\n",
    "\n",
    "inputs = tiny_custom_tokenizer(\n",
    "    prompt, return_tensors=\"pt\"\n",
    ").to(tiny_custom_model.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tiny_custom_tokenizer,\n",
    "    skip_prompt=True, \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "outputs = tiny_custom_model.generate(\n",
    "    **inputs, streamer=streamer,\n",
    "    use_cache=True, \n",
    "    max_new_tokens=128, \n",
    "    do_sample=False, \n",
    "    repetition_penalty=1.1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T13:08:25.505497Z",
     "start_time": "2024-07-29T13:08:22.600672Z"
    }
   },
   "id": "a8f6fba4f6629744",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def find_max(numbers):\n",
    "   max = 0\n",
    "   for num in numbers:\n",
    "       if num > max:\n",
    "           max = num\n",
    "   return max"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T13:08:25.509357Z",
     "start_time": "2024-07-29T13:08:25.507279Z"
    }
   },
   "id": "b8c4f84b22c0beab",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "7"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_max([1,3,5,1,6,7,2])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T13:08:26.301088Z",
     "start_time": "2024-07-29T13:08:26.276820Z"
    }
   },
   "id": "464e761b68426b77",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9a08bc0c6b677721"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
