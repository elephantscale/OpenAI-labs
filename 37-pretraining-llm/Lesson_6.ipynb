{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lesson 6: Model evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c6e0c4696ee8f23"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=./models/upstage/TinySolar-248m-4k \\\n",
    "    --tasks truthfulqa_mc2 \\\n",
    "    --device cpu \\\n",
    "    --limit 5"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2cfa60b275f75b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation for the Hugging Face Leaderboard"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d808ab8c12c0770c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def h6_open_llm_leaderboard(model_name):\n",
    "  task_and_shot = [\n",
    "      ('arc_challenge', 25),\n",
    "      ('hellaswag', 10),\n",
    "      ('mmlu', 5),\n",
    "      ('truthfulqa_mc2', 0),\n",
    "      ('winogrande', 5),\n",
    "      ('gsm8k', 5)\n",
    "  ]\n",
    "\n",
    "  for task, fewshot in task_and_shot:\n",
    "    eval_cmd = f\"\"\"\n",
    "    lm_eval --model hf \\\n",
    "        --model_args pretrained={model_name} \\\n",
    "        --tasks {task} \\\n",
    "        --device cpu \\\n",
    "        --num_fewshot {fewshot}\n",
    "    \"\"\"\n",
    "    os.system(eval_cmd)\n",
    "\n",
    "h6_open_llm_leaderboard(model_name=\"YOUR_MODEL\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e349b24d831eeab7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
