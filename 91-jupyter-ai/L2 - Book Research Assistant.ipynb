{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an AI Book Research Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides step-by-step instructions for replicating what Andrew did in the second demo. Please follow along, and feel free to play with different variations too! At the end, there are optional instructions for building a more advanced book research assistant than Andrew showed. \n",
    "\n",
    "*Useful Resources*:\n",
    "\n",
    "- **AISuite GitHub Repo**: [https://github.com/andrewyng/aisuite](https://github.com/andrewyng/aisuite)\n",
    "- **Open Library API**:\n",
    "  - [Open Library](https://openlibrary.org/developers/api) offers free and public Web APIs for accessing book and author catalog data (no API key is required)\n",
    "  - See `openlibrary_api_docs.md` in this lesson's directory for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Use Open Library API's Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Open Jupyter chat by clicking on the chat bubble icon on the left sidebar of Jupyter Lab\n",
    "- In the Chat interface, create a new chat by clicking `+Chat` (choose any name for the file)\n",
    "- In the chat window, attach the Markdown file `openlibrary_api_docs.md` using the `@` symbol:\n",
    "  - Type `@`, then select `file` from the autocomplete menu\n",
    "  - You'll see a list of available files in the lesson's directory, choose `openlibrary_api_docs.md` \n",
    "- Use a prompt like this:\n",
    "   > Write a code cell that uses Open Library's search endpoint to look up books based on a user's query and returns a response listing identified books, with these fields for each book: key, title, author_name, first_publish_year, isbn, cover_id, edition_count, language, subject. Use a variable called limit, set to 5, to specify the number of returned results. Print the results. Use the Open Library API documentation in the attached file.\n",
    "- Transfer the generated code to the cell below and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Wrap the API Call in a Tool Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the chat interface, use a prompt like this:\n",
    "  > Wrap the code above in a function `search_books`, with good docstring.\n",
    "  > \n",
    "  > - Input arguments: `query` and `limit` (with default value of 10)\n",
    "  > - Output: JSON string with:\n",
    "  >   - `num_found`: total results\n",
    "  >   - `books`: list of dicts with title, authors, first_publish_year, key, edition_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test the function by uncommenting the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = search_books(\"science fiction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build a Chat Handler with Automatic Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Drag and drop the raw cell below into the chat window and use a prompt like this:\n",
    "  > Use this aisuite example to create a simple chat handler function `chat_with_tools`:\n",
    "  > - input: `user_message`\n",
    "  > - output: `response.choices[0].message.content`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import aisuite as ai\n",
    "client = ai.Client()\n",
    "\n",
    "user_query = \"Suggest books about Science Fiction\"\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": user_query\n",
    "}]\n",
    "\n",
    "# Automatic tool execution with max_turns\n",
    "response = client.chat.completions.create(\n",
    "    model=\"openai:gpt-4.1-mini\",\n",
    "    messages=messages,\n",
    "    tools=[search_books],\n",
    "    max_turns=3  # Maximum number of back-and-forth tool calls\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test the Chat Handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run the following code cell to test `chat_with_tools`\n",
    "- Feel free to search for different types of books\n",
    "- Optional: If your generated function `search_books` does not contain printing statements, you can update the function as Andrew showed in the video:\n",
    "  - prompt used: modify the search_books function to add a printing statement showing when it is called with basic status messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXAMPLE: Science Function book search\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "user_query = \"Search for books about science fiction\"\n",
    "print(f\"\\nðŸ‘¤ User: {user_query}\")\n",
    "\n",
    "response = chat_with_tools(user_query)\n",
    "\n",
    "print(f\"\\nðŸ¤– Assistant: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Look up Details on Books (optional) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can repeat steps 1-3 to define a second tool for the book research assistant, so it can handle more complex book queries.\n",
    "For example, you can use the `works` endpoint to retrieve more detailed information about a particular book given its work id. For more details, check `openlibrary_api_docs.md`.\n",
    "\n",
    "Here are steps with suggested prompts:\n",
    "\n",
    "1. In the chat interface, attach the file containing the API documentation of Open Library (`openlibrary_api_docs.md`) and use a prompt like this:\n",
    "   > Write a code cell that uses Open Library's works endpoint to retrieve information about a book, based on the work's key that has this format: \"/works/{work_id}\". The result should include: title, description, subjects (limit to 10), first_sentence, covers (limit to 3). Print the returned results. Use the documentation for Open Library's API provided above.\n",
    "2. In the chat interface, create the function tool using a prompt like this:\n",
    "   > Wrap the code above in a function `get_book_details`:\n",
    "   > - Function argument: `work_key` in this format: `/works/{work_id}`\n",
    "   > - Output: JSON string with: title, description, subjects (limit to 10), first_sentence, covers (limit to 3)\n",
    "   >  - Write good docstring\n",
    "3. Update the `chat_with_tools` function to include the second tool as follows:\n",
    "   \n",
    "   ``` python\n",
    "   response = client.chat.completions.create(\n",
    "    model=\"openai:gpt-4.1-mini\",\n",
    "    messages=messages,\n",
    "    tools=[search_books, get_book_details],\n",
    "    max_turns=3  # Maximum number of back-and-forth tool calls\n",
    "   )\n",
    "   ```\n",
    "   \n",
    "4. Try this query:\n",
    "   ```python\n",
    "   user_query = \"Search for books with 'Linear Algebra' in the title and tell me about the one with the most editions\"\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
