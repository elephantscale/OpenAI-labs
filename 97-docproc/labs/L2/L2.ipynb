{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0b4ec12-fbcb-48f2-80e9-4310bdb3f3df",
   "metadata": {},
   "source": [
    "# Lab 1: Document Processing with OCR\n",
    "\n",
    "In this lesson, you will build an agent for document processing with optical character recognition (OCR). \n",
    "\n",
    "**Learning Objectives:**\n",
    "- Parsing and extracting information from documents \n",
    "- Building an agent equipped with an OCR tool\n",
    "- Identifying failure modes of OCR\n",
    "\n",
    "## Background\n",
    "\n",
    "Document processing converts unstructured documents meant for humans into structured data meant for machines. When documents are scanned images, OCR converts pixels to text. However, OCR alone produces raw text. Using LLMs, we can make sense of it.  \n",
    "\n",
    "## Outline\n",
    "\n",
    "- [1. Import Libraries](#1)\n",
    "- [2. Create OCR Tool](#2)\n",
    "  - [2.1. Use OCR to Parse a Sample Document](#2-1)\n",
    "  - [2.2. Use Regex to Extract Information](#2-2)\n",
    "- [3. Create the Agent](#3)\n",
    "- [4. Run the Agent and Extract Information](#4)\n",
    "- [5. OCR Limitations](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "## 1. Import Libraries\n",
    "\n",
    "Import the packages needed for the document processing system:\n",
    "\n",
    "- **Pillow (PIL)**: Load and manipulate images\n",
    "- **Pytesseract**: Python wrapper for Tesseract OCR engine\n",
    "- **LangChain**: Framework for building tool-enabled LLM applications\n",
    "- **OpenAI models**: LLM that provides reasoning capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dfa9ce-a207-4ba8-a368-688a61ffbab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import pytesseract\n",
    "\n",
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. Create OCR Tool\n",
    "\n",
    "The @tool decorator from LangChain transforms a function into a tool for use by agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedbe79b-eb9a-4354-a7eb-0e965a794bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def ocr_read_document(image_path: str) -> str:\n",
    "    \"\"\"Reads an image from the given path and returns extracted text using OCR.\"\"\"\n",
    "    try:\n",
    "        text = pytesseract.image_to_string(Image.open(image_path))\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"Error reading image: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-1",
   "metadata": {},
   "source": [
    "<a id=\"2-1\"></a>\n",
    "\n",
    "### 2.1. Use OCR to Parse a Sample Document\n",
    "\n",
    "Here's a clean digital invoice with crisp fonts, no handwriting, and no shadows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb88358-7f11-4185-bf71-0b8e4f0c0edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "image_path = 'invoice.png'\n",
    "img = Image.open(image_path)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ocr-output-note",
   "metadata": {},
   "source": [
    "Now run the OCR tool on the invoice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f64b55f-2996-41f0-a921-4562bf3da4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_text = ocr_read_document.run(\"invoice.png\")\n",
    "print(\"Raw OCR Output:\\n----------------\\n\", ocr_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-2",
   "metadata": {},
   "source": [
    "<a id=\"2-2\"></a>\n",
    "\n",
    "### 2.2. Use Regex to Extract Information\n",
    "\n",
    "Let's use regular expressions (regex) to extract tax and total from the parsed invoice. The patterns will look for \"Tax\" and \"Total\" to capture the respective numbers allowing for optional spacing and dollar sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae8027-8ce7-41c9-b355-998f05fd460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Regex to find Tax or Total\n",
    "tax_match = re.search(r'Tax\\s*\\$?([0-9.,]+)', ocr_text)\n",
    "total_match = re.search(r'Total\\s*\\$?([0-9.,]+)', ocr_text)\n",
    "\n",
    "# Print tax amount if found\n",
    "if tax_match:\n",
    "    print(\"Tax found by regex:\", tax_match.group(1))\n",
    "else:\n",
    "    print(\"Tax not found by regex.\")\n",
    "\n",
    "# Print total amount if found\n",
    "if total_match:\n",
    "    print(\"Total found by regex:\", total_match.group(1))\n",
    "else:\n",
    "    print(\"Total not found by regex.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regex-result-note",
   "metadata": {},
   "source": [
    "The regex pattern for \"Tax\" failed because the OCR output had \"Tax @\", not just \"Tax.\"\n",
    "\n",
    "The regex pattern matched \"Sub Total\" instead of \"Total\" because it appeared first and fit the pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## 3. Create the Agent\n",
    "\n",
    "Let's build an agent to parse and extract information from the invoice. For parsing, you will use OCR as a tool. For extracting, you will use LLMs as opposed to regex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baff1daf-9a44-4851-bc56-c44397c9580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env\n",
    "_ = load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c9e991-ae41-4305-af95-2a0380476bb1",
   "metadata": {},
   "source": [
    "Here we load the API key for OpenAI from a configuration file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70365c7-3ea1-4084-8680-986e4ff761db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the list of tools\n",
    "tools = [ocr_read_document]\n",
    "\n",
    "# 2. Set up the OpenAI model\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-5-mini\", \n",
    "    temperature=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-setup-note",
   "metadata": {},
   "source": [
    "Now create the agent using the ReAct framework with the LLM, tools, and prompt. Then set up the `AgentExecutor` to run the agent loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e8b58-bc52-4702-8ab5-15bb2e17022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "# 3. Create the OpenAI-compatible prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant designed to extract information from documents.\" \n",
    "            \"You have access to this tool: \"\n",
    "            \"OCR tool to extract raw text from images \"\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 4. Create a proper tool-calling agent\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "# 5. Set up the AgentExecutor to run the tool-enabled loop\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "\n",
    "## 4. Run the Agent and Extract Information\n",
    "\n",
    "Having implemented the agent, we can pass it a prompt for parsing and extracting information from the invoice.\n",
    "\n",
    "\n",
    "During the agent loop, the LLM decides when to call the OCR tool, receives the text output, and continues reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606506c3-0e0c-4ed2-a34b-1d7764abe563",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"\"\"\n",
    "Please process the document at 'invoice.png' using the OCR tool \n",
    "and extract the following information in JSON format:\n",
    "- tax\n",
    "- total\n",
    "\"\"\"\n",
    "\n",
    "# Use .invoke() with a dictionary input for the agent_executor\n",
    "response = agent_executor.invoke({\"input\": task})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-result-note",
   "metadata": {},
   "source": [
    "The agent returns JSON with the correct tax and total (not the subtotal). The LLM's comprehension capabilities enable the agent to extract information correctly based on the OCR output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "\n",
    "## 5. OCR Limitations\n",
    "\n",
    "The combination of OCR for parsing and regex for extracting worked on the previous example. However real-world documents can have complex layout or low resolution. Let's explore three examples. \n",
    "\n",
    "### 5.1. Complex Tables\n",
    "\n",
    "Tables are difficult for OCR. Misalignment of entries with unclear delineation between rows and columns can lead to poor results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08d5c2a-6e78-4feb-b650-70cdc917764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'table.png'\n",
    "img = Image.open(image_path)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "table-task-note",
   "metadata": {},
   "source": [
    "Extract the training cost in FLOPs from the EN-DE column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dea042-c930-41c7-bc74-8b5e0285a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"\"\"\n",
    "Extract the Training Cost (FLOPs) for EN-DE for ALL methods from the table.png \n",
    "using the OCR tool.\n",
    "Return as a list with model name and its training cost.\n",
    "\"\"\"\n",
    "\n",
    "# First, get the raw OCR output\n",
    "ocr_output = ocr_read_document.run(\"table.png\")\n",
    "\n",
    "# Use .invoke() with a dictionary input for the agent_executor\n",
    "response = agent_executor.invoke({\"input\": task})\n",
    "\n",
    "# Display results side by side\n",
    "print(\"=\"*80)\n",
    "print(\" \" * 22 + \"OCR TABLE RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\" + \"─\"*35 + \" OCR OUTPUT \" + \"─\"*33)\n",
    "print(ocr_output[:600] + \"...\" if len(ocr_output) > 600 else ocr_output)\n",
    "print(\"\\n\" + \"─\"*35 + \" LLM RESULT \" + \"─\"*33)\n",
    "print(response[\"output\"])\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "table-result-note",
   "metadata": {},
   "source": [
    "There are several issues with the results: \n",
    "\n",
    "- Exponents become apostrophes\n",
    "- Decimals become exclamation points\n",
    "- Columns are misaligned\n",
    "\n",
    "While the LLM can recognize patterns like empty cells, it may extract incorrect values from wrong columns due to garbled OCR output.\n",
    "\n",
    "### 5.2. Handwriting\n",
    "\n",
    "OCR struggles with handwriting. This is a fill-in-the-blanks exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b83ff4-4f52-41f2-bd09-a31bd60410e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'fill_in_the_blanks.jpg'\n",
    "img = Image.open(image_path)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handwriting-task-note",
   "metadata": {},
   "source": [
    "Extract the student name and all answers from the handwritten worksheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6f7247-21ba-46c3-b6d2-8222ec712af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"\"\"\n",
    "Please process the document at 'fill_in_the_blanks.jpg' using ocr \n",
    "and extract the following information in JSON format:\n",
    "- `student name`\n",
    "- `student answer to all the ten questions`\n",
    "\"\"\"\n",
    "\n",
    "# First, get the raw OCR output\n",
    "ocr_output = ocr_read_document.run(\"fill_in_the_blanks.jpg\")\n",
    "\n",
    "# Use .invoke() with a dictionary input for the agent_executor\n",
    "response = agent_executor.invoke({\"input\": task})\n",
    "\n",
    "# Display results side by side\n",
    "print(\"=\"*80)\n",
    "print(\" \" * 22 + \"HANDWRITTEN WORKSHEET RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\" + \"─\"*35 + \" OCR OUTPUT \" + \"─\"*33)\n",
    "print(ocr_output[:600] + \"...\" if len(ocr_output) > 600 else ocr_output)\n",
    "print(\"\\n\" + \"─\"*35 + \" LLM RESULT \" + \"─\"*33)\n",
    "print(response[\"output\"])\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a13d5ba-24a1-48a2-8497-581ef1220adf",
   "metadata": {},
   "source": [
    "### 5.3. Low-Quality Receipts\n",
    "Receipts are messy owing to low resolution, thermal printing, misaligned text, and shadows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b632add6-f75d-47b8-a481-0eb139986d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'receipt.jpg'\n",
    "img = Image.open(image_path)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "receipt-task-note",
   "metadata": {},
   "source": [
    "Ask the agent to verify if the total on the receipt is mathematically correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87976ed1-ab9e-4049-b497-0c89e6e6fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"\"\"\n",
    "Please process the document at 'receipt.jpg' and evaluate the correctness \n",
    "of the total.\n",
    "\"\"\"\n",
    "\n",
    "# Use .invoke() with a dictionary input for the agent_executor\n",
    "response = agent_executor.invoke({\"input\": task})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "receipt-result-note",
   "metadata": {},
   "source": [
    "Look at the first line item. The actual value is \\\\$7.95. But OCR misreads it as \\\\$7.99. This small numerical error throws off the entire total calculation. The answer is incorrect because the OCR tool didn't capture the numbers accurately. \n",
    "\n",
    "## Summary\n",
    "\n",
    "Here's what we covered:\n",
    "\n",
    "|  | Applications | Limitations |\n",
    "|-----------|----------|------------|\n",
    "| **OCR** | Good at parsing simple text | Bad at tables, handwriting, low-quality scans |\n",
    "| **Regex** | Good at pattern matching | Bad at handling variations in text |\n",
    "| **Agent** | Can adapt to variations in text | Dependent on OCR quality; otherwise may hallucinate |\n",
    "\n",
    "In the next lesson, we will explore how OCR has evolved over four decades and why that evolution matters for document processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
