{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MosesAI project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')\n",
    "MODEL=\"gpt-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "C_-uyi19vNjG",
    "outputId": "6f1fea63-559c-4763-b120-cbbd81a8291a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mark/anaconda3/envs/OpenAI/lib/python3.10/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "#from PIL import Image               # to load images\n",
    "#from IPython.display import display # to display images\n",
    "#pil_im = Image.open('/content/langchain and pinecone.png')\n",
    "#display(pil_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aYpmmfQ-ZbIh"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade langchain openai  -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XbL5zdf9aG8a",
    "outputId": "60cf911c-3d70-4b71-edb0-4b04f8a6d6fa"
   },
   "outputs": [],
   "source": [
    "#!pip install unstructured -q\n",
    "#!pip install unstructured[local-inference] -q\n",
    "#!pip install detectron2@git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HwalYTVZoRlH",
    "outputId": "9f475bea-721c-479f-c1fb-ea0ad263c9b7"
   },
   "outputs": [],
   "source": [
    "#!apt-get install poppler-utils  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AssnWUVlxtH"
   },
   "source": [
    "https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/directory_loader.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fulYnj9nZr3n",
    "outputId": "2b9f6003-0b72-407a-de44-df9b7e216890"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7145"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "directory = '../data/talmud'\n",
    "\n",
    "def load_docs(directory):\n",
    "  loader = DirectoryLoader(directory)\n",
    "  documents = loader.load()\n",
    "  return documents\n",
    "\n",
    "documents = load_docs(directory)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwaCosqsogzw"
   },
   "source": [
    "https://python.langchain.com/en/latest/modules/indexes/text_splitters/getting_started.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5lF8jA6xZ0Hm",
    "outputId": "7fd4a04f-f80d-465b-a028-e8a611ebb9f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7231\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_docs(documents,chunk_size=1000,chunk_overlap=20):\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "  docs = text_splitter.split_documents(documents)\n",
    "  return docs\n",
    "\n",
    "docs = split_docs(documents)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tVcFjgHJZ8S9",
    "outputId": "c8011cb1-9cd9-4223-ad71-60bbc83f9cbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If a son cohabits with his mother who is a harlot, promising her a sheep, that sheep may not be brought as a sacrifice. She can't sue him for the sheep in court, because of \"one is subject only to the greater (death) penalty,\" so their deal is invalid, but the prohibition takes effect.\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6PiPwt-FaYwl"
   },
   "outputs": [],
   "source": [
    "#requires for open ai embedding\n",
    "#!pip install tiktoken -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F5GY9voPa0av",
    "outputId": "5fd540c3-8e46-4863-aa07-8fcde3e0d822"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key)\n",
    "\n",
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LXhIY5SrrRec"
   },
   "outputs": [],
   "source": [
    "#!pip install pinecone-client -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vySq5oI5sU5V"
   },
   "source": [
    "https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/pinecone.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "hfIpYLV-acks"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f2978d29-7bdb-450c-8d78-f6ccd57328c6\n"
     ]
    }
   ],
   "source": [
    "import pinecone \n",
    "from langchain.vectorstores import Pinecone\n",
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=os.getenv('PINECONE_API_KEY'), \n",
    "    environment=\"us-west4-gcp-free\"  # next to api key in console\n",
    ")\n",
    "\n",
    "print(os.getenv('PINECONE_API_KEY'))\n",
    "index_name = \"langchain-demo\"\n",
    "\n",
    "#index = Pinecone.from_documents(docs, embeddings, index_name=index_name)\n",
    "\n",
    "# if you already have an index, you can load it like this\n",
    "index = Pinecone.from_existing_index(index_name, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o5r7YLpbchAD",
    "outputId": "0433cb24-6adb-4daa-d629-337669379a0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='However, the Shema is not said when one actually goes to sleep or wakes up, but rather in the general period when people lie down and get up. When is this? In the evening - when the Kohanim, who were impure, are returning from the mikveh to eat priestly portion (terumah), that is, at nightfall. That is when the time to say the Shema in the evening begins, but when does it end? Rabbi Eliezer said, \"Until the end of the first watch, that is, the first part of the night.\" Rabbi Eliezer understands \"when you lie down\" as the time when people go to sleep. The Sages say until midnight, and Rabban Gamliel says until dawn. We can understand Rabban Gamliel, \\'when people lie down\" means when people are asleep. But the opinion of the Sages will require clarification.', metadata={'source': '../data/talmud/brachot2.txt'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_similiar_docs(query,k=5,score=False):\n",
    "  if score:\n",
    "    similar_docs = index.similarity_search_with_score(query,k=k)\n",
    "  else:\n",
    "    similar_docs = index.similarity_search(query,k=k)\n",
    "  return similar_docs\n",
    "\n",
    "query = \"When do you say Shema?\"\n",
    "similar_docs = get_similiar_docs(query)\n",
    "len(similar_docs)\n",
    "similar_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtF9QZSvbLD9"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DuevPx4dbI4W",
    "outputId": "76a1ed52-c785-40a2-a0e9-828837972c18"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mark/anaconda3/envs/OpenAI/lib/python3.10/site-packages/langchain/llms/openai.py:169: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/home/mark/anaconda3/envs/OpenAI/lib/python3.10/site-packages/langchain/llms/openai.py:696: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvqmkpJvss17"
   },
   "source": [
    "https://python.langchain.com/en/latest/use_cases/question_answering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "RqCE-C3Ubty0",
    "outputId": "f78b567f-5483-4ea9-90fb-3698ce1bf325"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One must say the Shema prayer in the morning, \"when people rise.\" This is when there is enough light to distinguish between the blue and the white wool (which is used in one\\'s tzitzit). Some say, he needs to distinguish between a wolf and a dog, and some - to recognize his friend from four steps away. The time to say the Shema ends at sunrise according to Rabbi Eliezer, but Rabbi Yehoshua says it ends three hours into the day.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "def get_answer(query):\n",
    "  similar_docs = get_similiar_docs(query)\n",
    "  # print(similar_docs)\n",
    "  answer =  chain.run(input_documents=similar_docs, question=query)\n",
    "  return  answer\n",
    "\n",
    "query = \"When to say Shema?\"  \n",
    "get_answer(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "RTnCqRJ3r1kf",
    "outputId": "07b97326-64bf-4e25-9f61-a2eef3081784"
   },
   "outputs": [],
   "source": [
    "query = \"what are the sacrifices for? \\\n",
    "Sacrifices are typically brought for mistakes or unintentional transgressions, as stated in Keritot 9. \\\n",
    "However, there are cases when one brings an offering for intentional acts, such as relations with a slavewoman designated for another, a nazir who went to the cemetery, and one who swore a false oath of testimony (also mentioned in Keritot 9) \\\n",
    "what about bird sacrifices?\"\n",
    "get_answer(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM+LlDQsGXPEHE1XTQC7Kf0",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
