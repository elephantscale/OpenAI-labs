{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Compare finetuned vs. non-finetuned models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7494a7fdeec8d29d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import lamini\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "lamini.api_key = os.getenv('LAMINI_API_KEY')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62aac1a41f6ec20b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use base model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d6e7493021fd935"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm=lamini.Lamini(\"meta-llama/Llama-2-7b-hf\")\n",
    "print(llm.generate(\"Tell me how to train my dog to sit\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6dce1390005a0bd",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Non finetune model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fbe1039c73e1337"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm.generate(\"taylor swift's best friend\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d1e7bfb0619a798",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm.generate(\"\"\"Agent: I'm here to help you with your Amazon deliver order.\n",
    "Customer: I didn't get my item\n",
    "Agent: I'm sorry to hear that. Which item was it?\n",
    "Customer: the blanket\n",
    "Agent:\"\"\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbdb359c9d8c8449",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use fine-tuned model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c97b0859b5380d33"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm_fine_tuned=lamini.Lamini(\"meta-llama/Llama-2-7b-chat-hf\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d0713b4fcc3a03f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm_fine_tuned.generate(\"Tell me how to train my dog to sit\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0bc5cf8b5d8ead2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm_fine_tuned.generate(\"taylor swift's best friend\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c912627482632302",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm_fine_tuned.generate(\"\"\"Agent: I'm here to help you with your Amazon deliver order.\n",
    "Customer: I didn't get my item\n",
    "Agent: I'm sorry to hear that. Which item was it?\n",
    "Customer: the blanket\n",
    "Agent:\"\"\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb4653b73ee61d27",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAMINI CONFIGURATION\n",
      "{}\n",
      "LAMINI CONFIGURATION\n",
      "{}\n",
      "LAMINI CONFIGURATION\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "llm_chat_gpt=lamini.Lamini(\"chat-gpt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-19T13:11:19.270074Z",
     "start_time": "2024-08-19T13:11:19.246910Z"
    }
   },
   "id": "8d5ab9c0089c9da2",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "APIError",
     "evalue": "API error {'detail': \"Currently this user has support for base models: ['EleutherAI/pythia-410m', 'EleutherAI/pythia-70m', 'hf-internal-testing/tiny-random-gpt2', 'meta-llama/Llama-2-13b-chat-hf', 'meta-llama/Llama-2-7b-chat-hf', 'meta-llama/Llama-2-7b-hf', 'meta-llama/Meta-Llama-3-8B-Instruct', 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'microsoft/phi-2', 'microsoft/Phi-3-mini-4k-instruct', 'mistralai/Mistral-7B-Instruct-v0.1', 'mistralai/Mistral-7B-Instruct-v0.2', 'mistralai/Mistral-7B-Instruct-v0.3', 'Qwen/Qwen2-7B-Instruct']. Need help? Email us at info@lamini.ai\"}",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m~/PycharmProjects/48-Finetuning-Large-Language-Models/lib/python3.12/site-packages/lamini/api/rest_requests.py:25\u001B[0m, in \u001B[0;36mmake_web_request\u001B[0;34m(key, url, http_method, json)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 25\u001B[0m     \u001B[43mresp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m requests\u001B[38;5;241m.\u001B[39mexceptions\u001B[38;5;241m.\u001B[39mHTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/PycharmProjects/48-Finetuning-Large-Language-Models/lib/python3.12/site-packages/requests/models.py:1024\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1023\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[0;32m-> 1024\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mHTTPError\u001B[0m: 561 Server Error:  for url: https://api.lamini.ai/v1/completions",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mAPIError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mllm_chat_gpt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mTell me how to train my dog to sit\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/48-Finetuning-Large-Language-Models/lib/python3.12/site-packages/lamini/api/lamini.py:46\u001B[0m, in \u001B[0;36mLamini.generate\u001B[0;34m(self, prompt, model_name, output_type, max_tokens, stop_tokens)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate\u001B[39m(\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     33\u001B[0m     prompt: Union[\u001B[38;5;28mstr\u001B[39m, List[\u001B[38;5;28mstr\u001B[39m]],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     37\u001B[0m     stop_tokens: Optional[List[\u001B[38;5;28mstr\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     38\u001B[0m ):\n\u001B[1;32m     39\u001B[0m     req_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmake_llm_req_map(\n\u001B[1;32m     40\u001B[0m         prompt\u001B[38;5;241m=\u001B[39mprompt,\n\u001B[1;32m     41\u001B[0m         model_name\u001B[38;5;241m=\u001B[39mmodel_name \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_name,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     44\u001B[0m         stop_tokens\u001B[38;5;241m=\u001B[39mstop_tokens,\n\u001B[1;32m     45\u001B[0m     )\n\u001B[0;32m---> 46\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minference_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msubmit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(prompt, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(result) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m     48\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m output_type \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/PycharmProjects/48-Finetuning-Large-Language-Models/lib/python3.12/site-packages/lamini/api/inference_queue.py:41\u001B[0m, in \u001B[0;36mInferenceQueue.submit\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# Wait for all the results to come back\u001B[39;00m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m result \u001B[38;5;129;01min\u001B[39;00m results:\n\u001B[0;32m---> 41\u001B[0m     \u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m# Combine the results and return them\u001B[39;00m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcombine_results(results)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:456\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    454\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[0;32m--> 456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    457\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    458\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:401\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[1;32m    400\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 401\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    403\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[1;32m    404\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/thread.py:58\u001B[0m, in \u001B[0;36m_WorkItem.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfuture\u001B[38;5;241m.\u001B[39mset_exception(exc)\n",
      "File \u001B[0;32m~/PycharmProjects/48-Finetuning-Large-Language-Models/lib/python3.12/site-packages/lamini/api/inference_queue.py:103\u001B[0m, in \u001B[0;36mprocess_batch\u001B[0;34m(key, api_prefix, batch)\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprocess_batch\u001B[39m(key, api_prefix, batch):\n\u001B[1;32m    102\u001B[0m     url \u001B[38;5;241m=\u001B[39m api_prefix \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompletions\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 103\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mmake_web_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpost\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/PycharmProjects/48-Finetuning-Large-Language-Models/lib/python3.12/site-packages/lamini/api/rest_requests.py:76\u001B[0m, in \u001B[0;36mmake_web_request\u001B[0;34m(key, url, http_method, json)\u001B[0m\n\u001B[1;32m     74\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m description \u001B[38;5;241m==\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdetail\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m}:\n\u001B[1;32m     75\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m APIError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m500 Internal Server Error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 76\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m APIError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAPI error \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdescription\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\u001B[38;5;241m.\u001B[39mjson()\n",
      "\u001B[0;31mAPIError\u001B[0m: API error {'detail': \"Currently this user has support for base models: ['EleutherAI/pythia-410m', 'EleutherAI/pythia-70m', 'hf-internal-testing/tiny-random-gpt2', 'meta-llama/Llama-2-13b-chat-hf', 'meta-llama/Llama-2-7b-chat-hf', 'meta-llama/Llama-2-7b-hf', 'meta-llama/Meta-Llama-3-8B-Instruct', 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'microsoft/phi-2', 'microsoft/Phi-3-mini-4k-instruct', 'mistralai/Mistral-7B-Instruct-v0.1', 'mistralai/Mistral-7B-Instruct-v0.2', 'mistralai/Mistral-7B-Instruct-v0.3', 'Qwen/Qwen2-7B-Instruct']. Need help? Email us at info@lamini.ai\"}"
     ]
    }
   ],
   "source": [
    "llm_chat_gpt.generate(\"Tell me how to train my dog to sit\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-19T13:11:48.395368Z",
     "start_time": "2024-08-19T13:11:46.196184Z"
    }
   },
   "id": "f1b04ff544d31807",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a976ea48a852a921"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
