{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81b7b224e9b148fa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc177d978ad0844d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import lamini\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "lamini.api_key = os.getenv(\"LAMINI_API_KEY\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "119d7aac5c974b9f",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the Lamini docs dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5ab2daff41e64ed"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "dataset_name = \"lamini_docs.jsonl\"\n",
    "dataset_path = \"lamini/lamini_docs\"\n",
    "use_hf = True"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e094650a1aae655",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set up the model, training config, and tokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f09f94acd78bb849"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/pythia-70m\"\n",
    "\n",
    "training_config = {\n",
    "    \"model\": {\n",
    "        \"pretrained_name\": model_name,\n",
    "        \"max_length\": 2048\n",
    "    },\n",
    "    \"datasets\": {\n",
    "        \"use_hf\": use_hf,\n",
    "        \"path\": dataset_path\n",
    "    },\n",
    "    \"verbose\": True\n",
    "\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6db1ed58d8f98ba5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from utilities import *\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c97084432a0dec2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the base model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "495b2e626caa6ca9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d40af4d09a42ed5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device_count = torch.cuda.device_count()\n",
    "\n",
    "if device_count > 0:\n",
    "    logger.debug(\"Select GPU device\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logger.debug(\"Select CPU device\")\n",
    "    device = torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "991c768b769ba53f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "base_model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efbd0a5d08c6c2b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define function to carry out inference"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3917adf50aae5ef0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "    # Tokenize\n",
    "    input_ids = tokenizer.encode(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_tokens\n",
    "    )\n",
    "\n",
    "    # Generate\n",
    "    device = model.device\n",
    "    generated_tokens_with_prompt = model.generate(\n",
    "        input_ids=input_ids.to(device),\n",
    "        max_length=max_output_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode\n",
    "    generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "    # Strip the prompt\n",
    "    generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "    return generated_text_answer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "632514e4f1e6dd56",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Try the base model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "906010084aaf1402"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_text = test_dataset[0]['question']\n",
    "print(\"Question input (test):\", test_text)\n",
    "print(f\"Correct answer from Lamini docs: {test_dataset[0]['answer']}\")\n",
    "print(\"Model's answer: \")\n",
    "print(inference(test_text, base_model, tokenizer))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ce8d46acd548201",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "983712fc7a76c084"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "max_steps = 3\n",
    "trained_model_name = f\"lamini_docs_{max_steps}_steps\"\n",
    "output_dir = trained_model_name\n",
    "training_args = TrainingArguments(\n",
    "\n",
    "    # Learning rate\n",
    "    learning_rate=1.0e-5,\n",
    "\n",
    "    # Number of training epochs\n",
    "    num_train_epochs=1,\n",
    "\n",
    "    # Max steps to train for (each step is a batch of data)\n",
    "    # Overrides num_train_epochs, if not -1\n",
    "    max_steps=max_steps,\n",
    "\n",
    "    # Batch size for training\n",
    "    per_device_train_batch_size=1,\n",
    "\n",
    "    # Directory to save model checkpoints\n",
    "    output_dir=output_dir,\n",
    "\n",
    "    # Other arguments\n",
    "    overwrite_output_dir=False,  # Overwrite the content of the output directory\n",
    "    disable_tqdm=False,  # Disable progress bars\n",
    "    eval_steps=120,  # Number of update steps between two evaluations\n",
    "    save_steps=120,  # After # steps model is saved\n",
    "    warmup_steps=1,  # Number of warmup steps for learning rate scheduler\n",
    "    per_device_eval_batch_size=1,  # Batch size for evaluation\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    optim=\"adafactor\",\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=False,\n",
    "\n",
    "    # Parameters for early stopping\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ee7b27b06f86d64",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_flops = (\n",
    "        base_model.floating_point_ops(\n",
    "            {\n",
    "                \"input_ids\": torch.zeros(\n",
    "                    (1, training_config[\"model\"][\"max_length\"])\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        * training_args.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "print(base_model)\n",
    "print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
    "print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95a00a51764c6607",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    model_flops=model_flops,\n",
    "    total_steps=max_steps,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "863916fd1fca09ff",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "training_output = trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12c0fe21a53a4dc0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save model locally"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2acfe545f24c0201"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "save_dir = f'{output_dir}/final'\n",
    "\n",
    "trainer.save_model(save_dir)\n",
    "print(\"Saved model to:\", save_dir)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e719024c35ed9b9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25754ed56d7a8c25",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "finetuned_slightly_model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "296ee3c18c2821ac",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_question = test_dataset[0]['question']\n",
    "print(\"Question input (test):\", test_question)\n",
    "\n",
    "print(\"Finetuned slightly model's answer: \")\n",
    "print(inference(test_question, finetuned_slightly_model, tokenizer))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d5cbb7ed9f3f6b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run slightly trained model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7004c2412aa86542"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_answer = test_dataset[0]['answer']\n",
    "print(\"Target answer output (test):\", test_answer)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "656b109ea17f6f42",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run same model trained for two epochs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbd2e7a617419bca"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "finetuned_longer_model = AutoModelForCausalLM.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
    "\n",
    "finetuned_longer_model.to(device)\n",
    "print(\"Finetuned longer model's answer: \")\n",
    "print(inference(test_question, finetuned_longer_model, tokenizer))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d03c03dd15108d0a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Run much larger trained model and explore moderation\n",
    "# \n",
    "# bigger_finetuned_model = BasicModelRunner(model_name_to_id[\"bigger_model_name\"])\n",
    "# bigger_finetuned_output = bigger_finetuned_model(test_question)\n",
    "# print(\"Bigger (2.8B) finetuned model (test): \", bigger_finetuned_output)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a68c93d776889b48",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(train_dataset)):\n",
    " if \"keep the discussion relevant to Lamini\" in train_dataset[i][\"answer\"]:\n",
    "  print(i, train_dataset[i][\"question\"], train_dataset[i][\"answer\"])\n",
    "  count += 1\n",
    "print(count)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90d931c21f811f78",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Explore moderation using small model\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "print(inference(\"What do you think of Mars?\", base_model, base_tokenizer))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aaeaed4ee3486ad6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# try moderation with finetuned small model\n",
    "print(inference(\"What do you think of Mars?\", finetuned_longer_model, tokenizer))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "347893c0da59c1d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Finetune a model in 3 lines of code using Lamini"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c85e96bebe1a3c2a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama import BasicModelRunner\n",
    "\n",
    "model = BasicModelRunner(\"EleutherAI/pythia-410m\") \n",
    "model.load_data_from_jsonlines(\"lamini_docs.jsonl\", input_key=\"question\", output_key=\"answer\")\n",
    "model.train(is_public=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "574f827de59d6702",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b66112e4f7f06af4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c7f42e0532cbf6f7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a0baa1a8673fdf4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
