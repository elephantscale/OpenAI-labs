{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc935f2b-a58b-4110-8918-96c868047b38",
   "metadata": {},
   "source": [
    "# L3: Supervised Fine-Tuning (SFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec12b0",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>(Kernel Starting)</code>:</b> This notebook takes about 30 seconds to be ready to use. You may start and watch the video while you wait.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76b86a7-7a45-4749-866e-0d561041bce1",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbfc986-9ac7-4a2d-9dd0-a76841c7f46d",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3304e49d-bd1e-469b-a5b4-5edb16ecf344",
   "metadata": {
    "height": 113
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139f2ab3",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> üíª &nbsp; <b>Access <code>requirements.txt</code> file:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>.</p>\n",
    "\n",
    "<p> ‚¨á &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "\n",
    "<p> üìí &nbsp; For more help, please see the <em>\"Appendix ‚Äì Tips, Help, and Download\"</em> Lesson.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc63b02-5e9a-4a83-b042-4a2386cf5976",
   "metadata": {},
   "source": [
    "## Setting up helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69982ae0-755e-48cf-ba4c-3b83b091fd9a",
   "metadata": {
    "height": 572
   },
   "outputs": [],
   "source": [
    "def generate_responses(model, tokenizer, user_message, system_message=None, \n",
    "                       max_new_tokens=100):\n",
    "    # Format chat using tokenizer's chat template\n",
    "    messages = []\n",
    "    if system_message:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "    \n",
    "    # We assume the data are all single-turn conversation\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False,\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    # Recommended to use vllm, sglang or TensorRT\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    generated_ids = outputs[0][input_len:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e5b05-a493-4683-91fd-7417885efc0f",
   "metadata": {
    "height": 164
   },
   "outputs": [],
   "source": [
    "def test_model_with_questions(model, tokenizer, questions, \n",
    "                              system_message=None, title=\"Model Output\"):\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    for i, question in enumerate(questions, 1):\n",
    "        response = generate_responses(model, tokenizer, question, \n",
    "                                      system_message)\n",
    "        print(f\"\\nModel Input {i}:\\n{question}\\nModel Output {i}:\\n{response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c273931-6827-4ee1-af1a-83a99bf94bf7",
   "metadata": {
    "height": 402
   },
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name, use_gpu = False):\n",
    "    \n",
    "    # Load base model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    if use_gpu:\n",
    "        model.to(\"cuda\")\n",
    "    \n",
    "    if not tokenizer.chat_template:\n",
    "        tokenizer.chat_template = \"\"\"{% for message in messages %}\n",
    "                {% if message['role'] == 'system' %}System: {{ message['content'] }}\\n\n",
    "                {% elif message['role'] == 'user' %}User: {{ message['content'] }}\\n\n",
    "                {% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }} <|endoftext|>\n",
    "                {% endif %}\n",
    "                {% endfor %}\"\"\"\n",
    "    \n",
    "    # Tokenizer config\n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd15e1d-6ecd-4337-a5dd-1602da354f62",
   "metadata": {
    "height": 334
   },
   "outputs": [],
   "source": [
    "def display_dataset(dataset):\n",
    "    # Visualize the dataset \n",
    "    rows = []\n",
    "    for i in range(3):\n",
    "        example = dataset[i]\n",
    "        user_msg = next(m['content'] for m in example['messages']\n",
    "                        if m['role'] == 'user')\n",
    "        assistant_msg = next(m['content'] for m in example['messages']\n",
    "                             if m['role'] == 'assistant')\n",
    "        rows.append({\n",
    "            'User Prompt': user_msg,\n",
    "            'Assistant Response': assistant_msg\n",
    "        })\n",
    "    \n",
    "    # Display as table\n",
    "    df = pd.DataFrame(rows)\n",
    "    pd.set_option('display.max_colwidth', None)  # Avoid truncating long strings\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ac817-43a4-43c9-88f3-9825b96b84b7",
   "metadata": {},
   "source": [
    "## Load base model & test on simple questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fed78c2-ea93-4ac2-bd6f-5d4391de7c8d",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "USE_GPU = False\n",
    "\n",
    "questions = [\n",
    "    \"Give me an 1-sentence introduction of LLM.\",\n",
    "    \"Calculate 1+1-1\",\n",
    "    \"What's the difference between thread and process?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba426c74-4d93-42b3-b2c7-5791fb9bf3c5",
   "metadata": {
    "height": 130
   },
   "outputs": [],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(\"./models/Qwen/Qwen3-0.6B-Base\", USE_GPU)\n",
    "\n",
    "test_model_with_questions(model, tokenizer, questions, \n",
    "                          title=\"Base Model (Before SFT) Output\")\n",
    "\n",
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885253e0-3a9a-4b42-a36b-a8a3ddd340a1",
   "metadata": {},
   "source": [
    "## SFT results on Qwen3-0.6B model\n",
    "\n",
    "In this section, we're reviewing the results of a previously completed SFT training. Due to limited resources, we won‚Äôt be running the full training on a relatively large model like Qwen3-0.6B. However, in the next section of this notebook, you‚Äôll walk through the full training process using a smaller model and a lightweight dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e86f13c-c969-4c7e-8702-d074ee7a2ce6",
   "metadata": {
    "height": 130
   },
   "outputs": [],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(\"./models/banghua/Qwen3-0.6B-SFT\", USE_GPU)\n",
    "\n",
    "test_model_with_questions(model, tokenizer, questions, \n",
    "                          title=\"Base Model (After SFT) Output\")\n",
    "\n",
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf299ee-aa84-4c43-8d7b-f0998077e2cb",
   "metadata": {},
   "source": [
    "## Doing SFT on a small model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5cb7ea-e157-418e-84f5-34ecbed823ad",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> üíª &nbsp; <b>Note:</b> We're performing SFT on a small model <code>HuggingFaceTB/SmolLM2-135M</code> and a smaller training dataset to to ensure the full training process can run on limited computational resources. If you're running the notebooks on your own machine and have access to a GPU, feel free to switch to a larger model‚Äîsuch as <code>Qwen/Qwen3-0.6B-Base</code>‚Äîto perform full SFT and reproduce the results shown above.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb07589-049d-432e-8001-e6e9175ad806",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "model_name = \"./models/HuggingFaceTB/SmolLM2-135M\"\n",
    "model, tokenizer = load_model_and_tokenizer(model_name, USE_GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d52c3e-9c6c-43c3-bd95-92d60b9c9a8f",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"banghua/DL-SFT-Dataset\")[\"train\"]\n",
    "if not USE_GPU:\n",
    "    train_dataset=train_dataset.select(range(100))\n",
    "\n",
    "display_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c515a8-3728-45fa-88cc-6eb4de839839",
   "metadata": {
    "height": 198
   },
   "outputs": [],
   "source": [
    "# SFTTrainer config \n",
    "sft_config = SFTConfig(\n",
    "    learning_rate=8e-5, # Learning rate for training. \n",
    "    num_train_epochs=1, #  Set the number of epochs to train the model.\n",
    "    per_device_train_batch_size=1, # Batch size for each device (e.g., GPU) during training. \n",
    "    gradient_accumulation_steps=8, # Number of steps before performing a backward/update pass to accumulate gradients.\n",
    "    gradient_checkpointing=False, # Enable gradient checkpointing to reduce memory usage during training at the cost of slower training speed.\n",
    "    logging_steps=2,  # Frequency of logging training progress (log every 2 steps).\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd7d132-4c69-4b12-a8ec-e6b4795faad9",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "sft_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=train_dataset, \n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "sft_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b4bac1-7262-4c55-b411-6a59188157b0",
   "metadata": {},
   "source": [
    "## Testing training results on small model and small dataset\n",
    "\n",
    "**Note:** The following results are for the small model and dataset we used for SFT training, due to limited computational resources. To view the results of full-scale training on a larger model, see the **\"SFT Results on Qwen3-0.6B Model\"** section above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d678274-5768-4cea-ae20-051488e5d0f3",
   "metadata": {
    "height": 96
   },
   "outputs": [],
   "source": [
    "if not USE_GPU: # move model to CPU when GPU isn‚Äôt requested\n",
    "    sft_trainer.model.to(\"cpu\")\n",
    "test_model_with_questions(sft_trainer.model, tokenizer, questions, \n",
    "                          title=\"Base Model (After SFT) Output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2d5896-6fd6-43d2-85f1-dacbd594f4cf",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3c0bba-984a-494c-8374-33db30ad1da6",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5521a00a-88d8-4ad9-b134-12ea94e76984",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecddf372-7382-41ed-bc3d-ead2eef33201",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9841d1c8-c4d5-43e9-9d1e-1f267057279a",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64a77d2-3338-4a8f-b634-d25be5b19407",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
