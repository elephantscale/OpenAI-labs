{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1021654a-a332-4104-be2b-6b1a32e2ddf1",
   "metadata": {},
   "source": [
    "# L5: Long-context understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9238c61d-9499-4d8e-bf74-9cf36b96f23b",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>(Kernel Starting)</code>:</b> This notebook takes about 30 seconds to be ready to use. You may start and watch the video while you wait.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b335f98f-c0aa-49fc-bedc-9c1f0bf6d7a0",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae630b5-a8ca-450c-a244-5d93cbb073a9",
   "metadata": {},
   "source": [
    "## Load API keys and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cccd58bb-ec91-49c0-bc82-5f8246f92472",
   "metadata": {
    "height": 232
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import get_llama_api_key, get_llama_base_url, get_together_api_key\n",
    "from utils import get_hf_access_token, get_github_access_token\n",
    "\n",
    "llama_api_key = get_llama_api_key()\n",
    "llama_base_url = get_llama_base_url()\n",
    "together_api_key = get_together_api_key()\n",
    "hf_access_token = get_hf_access_token()\n",
    "github_access_token = get_github_access_token()\n",
    "\n",
    "from utils import llama4, llama4_together\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda4d8fa-a041-4de2-b188-18bff717528b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> üíª &nbsp; <b>Access <code>requirements.txt</code> and <code>helper.py</code> files:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>.</p>\n",
    "\n",
    "<p> ‚¨á &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "\n",
    "<p> üìí &nbsp; For more help, please see the <em>\"Appendix ‚Äì Tips, Help, and Download\"</em> Lesson.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "229ccd49-5258-4646-8517-b1fad334acdf",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8b86aa8-d3b6-4d12-b8d7-fc7866675a23",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 7\n"
     ]
    }
   ],
   "source": [
    "prompt = \"This is a test prompt.\"\n",
    "num_tokens = len(tokenizer.encode(prompt))\n",
    "print(f\"Number of tokens: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7191466e-250f-404b-9c7c-9fb7b4cf6d48",
   "metadata": {},
   "source": [
    "## Large book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46a6a1a8-53d9-4ba1-9367-e45d62fd711c",
   "metadata": {
    "height": 79
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3227578, 778678)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"files/war-and-peace.txt\", \"r\", encoding='utf=8') as file:\n",
    "    wp = file.read()\n",
    "len(wp), len(tokenizer.encode(wp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8165bcb9-2300-444f-ab84-36a63db76651",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> üö®\n",
    "&nbsp; <b>Different Run Results:</b> The output generated by AI chat models can vary with each execution due to their dynamic, probabilistic nature. Don't be surprised if your results differ from those shown in the video.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6ec5c2f-c274-4f93-b181-e2f8be467525",
   "metadata": {
    "height": 45
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook of War and Peace by Leo Tolstoy is a classic novel that follows the lives of several aristocratic Russian families during the Napoleonic Wars. The story begins in July 1805 at a reception hosted by Anna Pavlovna Scherer, where the guests discuss the impending war with Napoleon. Prince Vasili Kuragin, a high-ranking and influential man, is the first to arrive at the reception. He is greeted by Anna Pavlovna, who is suffering from a cough. The conversation turns to politics, and Anna Pavlovna expresses her concerns about Napoleon's intentions.\n",
      "\n",
      "Meanwhile, Pierre Bezukhov, the illegitimate son of a wealthy count, arrives at the reception. He is a socially awkward but intelligent and observant young man. Anna Pavlovna is anxious about Pierre's presence, as he does not know how to behave in high society. The conversation turns to the topic of Napoleon's conquests and the reaction of the European sovereigns.\n",
      "\n",
      "The novel then follows the lives of several characters, including Prince Andrew Bolkonski, who joins the Russian army as an aide-de-camp to General Kutuzov. Prince Andrew is a handsome and ambitious young man who is disillusioned with the war. He meets Pierre, who is a friend from his past, and they discuss their views on life and the war.\n",
      "\n",
      "The story also follows the life of Natashya Rostova, a young and beautiful girl who is engaged to Prince Andrew. However, she begins to develop feelings for Anatole Kuragin, who is a charming but untrustworthy young man. Anatole is a womanizer who is secretly married and has a history of seducing women.\n",
      "\n",
      "As the war with Napoleon intensifies, the Russian army retreats, and the French army advances towards Moscow. The Russian army is led by General Kutuzov, who is determined to defend Russia against the French invasion. The French army, led by Napoleon, is confident of victory but is ultimately defeated by the Russian army and the harsh Russian winter.\n",
      "\n",
      "The novel explores themes of love, family, loyalty, and power, and it is considered one of the greatest works of literature ever written. The characters are complex and multi-dimensional, and their stories are woven together to create a rich and detailed portrait of Russian society during the Napoleonic Wars.\n",
      "\n",
      "The novel also explores the concept of free will and inevitability, and how individuals make choices that are influenced by their circumstances and the world around them. The author argues that true greatness lies not in the actions of individuals, but in the collective efforts of people and the laws that govern human behavior.\n",
      "\n",
      "Throughout the novel, Tolstoy draws on his own experiences and observations of life, and he creates a vivid and detailed picture of Russian society during the Napoleonic Wars. The novel is a sweeping epic that explores the human condition and the nature of history itself.\n",
      "\n",
      "The characters in the novel are complex and multi-dimensional, and they undergo significant changes throughout the story. Prince Andrew, for example, begins as a proud and ambitious young man but eventually becomes disillusioned with the war and finds a new sense of purpose. Natashya, on the other hand, begins as a young and carefree girl but eventually becomes a strong and determined woman.\n",
      "\n",
      "The novel also explores the themes of love, family, and loyalty, and it shows how these themes are intertwined with the larger historical events of the time. The characters' personal relationships and experiences are deeply affected by the war, and they must navigate the challenges and uncertainties of life during a time of great upheaval.\n",
      "\n",
      "Overall, War and Peace is a masterpiece of literature that explores the human condition and the nature of history itself. It is a rich and detailed portrait of Russian society during the Napoleonic Wars, and it continues to be widely read and studied today.\n"
     ]
    }
   ],
   "source": [
    "print(llama4_together(f\"give me a summary of the book below: {wp}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d6e4a5-cacf-405f-a35c-40d1a5f7dcbf",
   "metadata": {},
   "source": [
    "## Multiple document summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a28c9e9-1cd5-4b2b-9416-7aafb9297066",
   "metadata": {
    "height": 232
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2407.16833.pdf 48830 13393\n",
      "2409.01666.pdf 17822 4886\n",
      "2411.03538.pdf 49248 13122\n",
      "2503.17407.pdf 426588 108877\n",
      "2404.06654.pdf 84506 28690\n",
      "Total papers: 627047,\n",
      "168986\n"
     ]
    }
   ],
   "source": [
    "from utils import pdf2text\n",
    "\n",
    "papers = [\"2407.16833.pdf\", \"2409.01666.pdf\", \"2411.03538.pdf\", \"2503.17407.pdf\", \"2404.06654.pdf\"]\n",
    "paper_texts = []\n",
    "for n, paper in enumerate(papers):\n",
    "  text = pdf2text(f\"files/{paper}\")\n",
    "  paper_texts.append(f\"Paper {n+1}:\\n{text}\")\n",
    "  print(paper, len(text), len(tokenizer.encode(text)))\n",
    "\n",
    "total_text = \"\\n\\n\".join(paper_texts)\n",
    "print(f\"\"\"Total papers: {len(total_text)},\n",
    "{len(tokenizer.encode(total_text))}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bb825fe-d586-4a92-9c92-2ec0c2690e50",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Paper 1: Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach\n",
      "\n",
      "The paper presents a comprehensive study comparing Retrieval Augmented Generation (RAG) and long-context Large Language Models (LLMs). The authors evaluate RAG and long-context LLMs on various public datasets using three latest LLMs: Gemini-1.5-Pro, GPT-4O, and GPT-3.5-Turbo. The results show that long-context LLMs consistently outperform RAG in terms of average performance. However, RAG remains relevant due to its significantly lower computational cost. The authors propose SELF-ROUTE, a simple yet effective method that routes queries to RAG or long-context LLMs based on model self-reflection. SELF-ROUTE significantly reduces the computation cost while maintaining comparable performance to long-context LLMs.\n",
      "\n",
      "## Paper 2: In Defense of RAG in the Era of Long-Context Language Models\n",
      "\n",
      "The paper argues that Retrieval Augmented Generation (RAG) is still necessary in the era of long-context language models. The authors propose an order-preserve retrieval-augmented generation (OP-RAG) mechanism, which preserves the order of retrieved chunks in the original text. The experiments demonstrate that OP-RAG achieves higher answer quality compared to long-context LLMs without RAG. The authors also show that OP-RAG can achieve higher F1 scores even with a significant reduction in input length.\n",
      "\n",
      "## Paper 3: Long Context RAG Performance of Large Language Models\n",
      "\n",
      "The paper presents a comprehensive study of the impact of increased context length on Retrieval Augmented Generation (RAG) performance across 20 popular open-source and commercial Large Language Models (LLMs). The authors evaluate the performance of LLMs on three domain-specific datasets, varying the total context length from 2,000 to 128,000 tokens. The results show that using longer context does not uniformly increase RAG performance, and only a handful of the most recent state-of-the-art LLMs can maintain consistent accuracy at long context above 64k tokens.\n",
      "\n",
      "## Paper 4: M-A-P: A Comprehensive Survey on Long Context Language Modeling\n",
      "\n",
      "The paper presents a comprehensive survey on recent advances in long-context modeling for large language models. The authors discuss data strategies, architectural designs, and workflow approaches oriented with long context processing. The survey covers three key aspects: how to obtain effective and efficient long-context language models, how to train and deploy them efficiently, and how to evaluate and analyze them comprehensively.\n",
      "\n",
      "## Paper 5: RULER: What's the Real Context Size of Your Long-Context Language Models?\n",
      "\n",
      "The paper proposes RULER, a new benchmark to evaluate long-context language models via synthetic tasks with flexible configurations. RULER contains four task categories: retrieval, multi-hop tracing, aggregation, and question answering. The authors evaluate 17 long-context LLMs using RULER and perform analysis across models and task complexities. The results show that despite achieving nearly perfect performance on the vanilla needle-in-a-haystack test, almost all models exhibit large performance drops as the context length increases. The authors open-source RULER to spur future research in long-context language models.\n"
     ]
    }
   ],
   "source": [
    "print(llama4_together(f\"\"\"give me a summary of the five papers\n",
    "below: {total_text}\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe601f77-2902-46cb-b37b-1e61a06cca0d",
   "metadata": {},
   "source": [
    "## Single-hop codebase question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "378a2ff9-ea3e-481f-b215-ab342340d2ee",
   "metadata": {
    "height": 283
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./llama-models/llama-models-main/models/__init__.py\n",
      "Writing ./llama-models/llama-models-main/models/checkpoint.py\n",
      "Writing ./llama-models/llama-models-main/models/datatypes.py\n",
      "Writing ./llama-models/llama-models-main/models/quantize_impls.py\n",
      "Writing ./llama-models/llama-models-main/models/tokenizer_utils.py\n",
      "Writing ./llama-models/llama-models-main/models/llama3/__init__.py\n",
      "Writing ./llama-models/llama-models-main/models/llama3/args.py\n",
      "Writing ./llama-models/llama-models-main/models/llama3/chat_format.py\n",
      "Writing ./llama-models/llama-models-main/models/llama3/generation.py\n",
      "Writing ./llama-models/llama-models-main/models/llama3/model.py\n",
      "Writing ./llama-models/llama-models-main/models/llama3/tokenizer.py\n",
      "Writing ./llama-models/llama-models-main/models/llama3/tool_utils.py\n",
      "Writing ./llama-models/llama-models-main/models/llama3/multimodal/__init__.py\n",
      "Writing ./llama-models/llama-models-main/models/llama3/multimodal/encoder_utils.py\n",
      "Writing ./llama-models/llama-models-main/models/llama3/multimodal/image_transform.py\n",
      "Writing ./llama-models/llama-models-main/models/llama3/multimodal/model.py\n",
      "Writing ./llama-models/llama-models-main/models/llama3/multimodal/utils.py\n",
      "Writing ./llama-models/llama-models-main/models/llama3/quantization/loader.py\n",
      "Writing ./llama-models/llama-models-main/models/llama3/scripts/__init__.py\n",
      "Writing ./llama-models/llama-models-main/models/llama3/scripts/chat_completion.py\n",
      "Writing ./llama-models/llama-models-main/models/llama3/scripts/completion.py\n",
      "Writing ./llama-models/llama-models-main/models/llama3/tests/api/test_generation.py\n",
      "Writing ./llama-models/llama-models-main/models/llama3/tests/api/test_tokenizer.py\n",
      "Writing ./llama-models/llama-models-main/models/llama3/tests/api/test_tool_utils.py\n",
      "Writing ./llama-models/llama-models-main/models/llama4/__init__.py\n",
      "Writing ./llama-models/llama-models-main/models/llama4/args.py\n",
      "Writing ./llama-models/llama-models-main/models/llama4/chat_format.py\n",
      "Writing ./llama-models/llama-models-main/models/llama4/datatypes.py\n",
      "Writing ./llama-models/llama-models-main/models/llama4/ffn.py\n",
      "Writing ./llama-models/llama-models-main/models/llama4/generation.py\n",
      "Writing ./llama-models/llama-models-main/models/llama4/model.py\n",
      "Writing ./llama-models/llama-models-main/models/llama4/moe.py\n",
      "Writing ./llama-models/llama-models-main/models/llama4/preprocess.py\n",
      "Writing ./llama-models/llama-models-main/models/llama4/tokenizer.py\n",
      "Writing ./llama-models/llama-models-main/models/llama4/quantization/__init__.py\n",
      "Writing ./llama-models/llama-models-main/models/llama4/quantization/loader.py\n",
      "Writing ./llama-models/llama-models-main/models/llama4/scripts/chat_completion.py\n",
      "Writing ./llama-models/llama-models-main/models/llama4/scripts/completion.py\n",
      "Writing ./llama-models/llama-models-main/models/llama4/scripts/quantize.py\n",
      "Writing ./llama-models/llama-models-main/models/llama4/vision/embedding.py\n",
      "Writing ./llama-models/llama-models-main/models/llama4/vision/encoder.py\n",
      "Output written to files/llama-models_files.txt\n"
     ]
    }
   ],
   "source": [
    "from utils import download_and_extract_repo, get_py_files, write_files_to_text\n",
    "\n",
    "repo_url = \"https://github.com/meta-llama/llama-models\"\n",
    "repo_name = repo_url.rstrip('/').split('/')[-1]\n",
    "extract_dir = f\"./{repo_name}\"\n",
    "\n",
    "download_repo = False # The repo is already downloaded and extracted on the platform. If you wouldf like to try this for another repo, please change this to True.\n",
    "if download_repo:\n",
    "    download_and_extract_repo(repo_url, extract_dir)\n",
    "\n",
    "py_files = get_py_files(extract_dir)\n",
    "output_file = f\"files/{repo_name}_files.txt\"\n",
    "write_files_to_text(py_files, output_file)\n",
    "\n",
    "print(f\"Output written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33250074-4e92-43bc-b436-5646da901086",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: ./llama-models/llama-models-main/models/__init__.py\r\n",
      "Content:\r\n",
      "# Copyright (c) Meta Platforms, Inc. and affiliates.\r\n",
      "# All rights reserved.\r\n",
      "#\r\n",
      "# This source code is licensed under the terms described in the LICENSE file in\r\n",
      "# top-level folder for each specific model found within the models/ directory at\r\n",
      "# the top-level of this source tree.\r\n",
      "\r\n",
      "\r\n",
      "Path: ./llama-models/llama-models-main/models/checkpoint.py\r\n",
      "Content:\r\n",
      "# Copyright (c) Meta Platforms, Inc. and affiliates.\r\n",
      "# All rights reserved.\r\n",
      "#\r\n",
      "# This source code is licensed under the terms described in the LICENSE file in\r\n",
      "# top-level folder for each specific model found within the models/ directory at\r\n",
      "# the top-level of this source tree.\r\n",
      "\r\n",
      "import concurrent.futures\r\n",
      "import re\r\n",
      "from pathlib import Path\r\n",
      "from typing import Any, Dict, List, Optional, Union\r\n",
      "\r\n",
      "import numpy as np\r\n",
      "import torch\r\n",
      "from fairscale.nn.model_parallel.initialize import get_model_parallel_rank, get_model_parallel_world_size\r\n",
      "\r\n",
      "\r\n",
      "def map_mp_rank(old_mp_size: int, new_mp_size: int, new_mp_rank: int) -> List[int]:\r\n",
      "    \"\"\"Map a new MP rank to a list of old MP ranks given a change in MP size.\"\"\"\r\n",
      "    if new_mp_size % old_mp_size == 0:\r\n",
      "        # Read old MP shard and split it into smaller ones\r\n",
      "        return [new_mp_rank * old_mp_size // new_mp_size]\r\n",
      "    elif old_mp_size % new_mp_size == 0:\r\n",
      "        # Merge old MP shards into a single one\r\n",
      "        mp_factor = old_mp_size // new_mp_size\r\n",
      "        return list(range(new_mp_rank * mp_factor, (new_mp_rank + 1) * mp_factor))\r\n",
      "    else:\r\n",
      "        raise ValueError(\r\n",
      "            f\"Either old MP size or new MP size should be a multiple of the other: \"\r\n",
      "            f\"{old_mp_size} % {new_mp_size} != 0 and {new_mp_size} % {old_mp_size} != 0\"\r\n",
      "        )\r\n",
      "\r\n",
      "\r\n",
      "def maybe_reshard_state_dict(\r\n",
      "    ckpt_paths: List[Path],\r\n",
      "    n_kv_heads: int,\r\n",
      "    moe_num_experts: Optional[int] = None,\r\n",
      "    map_location: Union[str, torch.device] = \"cpu\",\r\n",
      "    mmap: bool = True,\r\n",
      ") -> Dict[str, torch.Tensor]:\r\n",
      "    if str(map_location) == \"cpu\":\r\n",
      "        torch.set_default_tensor_type(torch.BFloat16Tensor)\r\n",
      "    else:\r\n",
      "        torch.set_default_tensor_type(torch.cuda.BFloat16Tensor)\r\n",
      "\r\n",
      "    ckpt_paths = np.array(sorted(ckpt_paths))\r\n",
      "\r\n",
      "    new_mp_size, new_mp_rank = get_model_parallel_world_size(), get_model_parallel_rank()\r\n",
      "    old_mp_size = len(ckpt_paths)\r\n",
      "    old_mp_ranks = map_mp_rank(old_mp_size, new_mp_size, new_mp_rank)\r\n",
      "\r\n",
      "    print(f\"Loading checkpoint shards:\\n{str(ckpt_paths[old_mp_ranks])}\")  # type: ignore\r\n",
      "    paths = ckpt_paths[old_mp_ranks]  # type: ignore\r\n",
      "    state_dicts = [torch.load(str(p), map_location=map_location, mmap=mmap) for p in paths]\r\n",
      "\r\n",
      "    if new_mp_size == old_mp_size:\r\n",
      "        return state_dicts[0]  # type: ignore\r\n",
      "\r\n",
      "    if moe_num_experts is not None:\r\n",
      "        state_dicts = [convert_moe_weights(d, moe_num_experts) for d in state_dicts]\r\n",
      "\r\n",
      "    print(f\"Resharding {len(state_dicts)} state dicts from MP size {old_mp_size} to MP size {new_mp_size}\")\r\n",
      "    return reshard_mp(\r\n",
      "        state_dicts,\r\n",
      "        size=max(new_mp_size // old_mp_size, 1),\r\n",
      "        rank=new_mp_rank % max(new_mp_size // old_mp_size, 1),\r\n",
      "        repeat_qk_qv=max(new_mp_size // n_kv_heads, 1),\r\n",
      "    )\r\n",
      "\r\n",
      "\r\n",
      "_WEIGHT_ROW_KEY = {\r\n",
      "    \"feed_forward.w2\",\r\n",
      "    \"feed_forward.mlp.fc2\",\r\n",
      "    \"attention.wo\",\r\n",
      "    \"feed_forward.mlp.fc2_weight\",\r\n",
      "    \"feed_forward.w_out_shared_DF.weight\",\r\n",
      "    \"attn.wo.weight\",\r\n",
      "    \"mlp.c_proj.weight\",\r\n",
      "}\r\n",
      "_MOE_WEIGHT_ROW_KEY = {\"feed_forward.experts.(moe_w_in_eD_F|moe_w_swiglu_eD_F)\"}\r\n",
      "\r\n",
      "_WEIGHT_COLUMN_KEY = {\r\n",
      "    \"output\",\r\n",
      "    \"feed_forward.(w1|w3)\",\r\n",
      "    \"feed_forward.mlp.(fc1|fc3)\",\r\n",
      "    \"feed_forward.mlp.fc1_weight\",\r\n",
      "    \"attention.(wk|wq|wv|wqkv).weight\",\r\n",
      "    \"feed_forward.(w_in_shared_FD|w_swiglu_FD)\",\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!head -100 files/llama-models_files.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff7635eb-da7a-4ded-9bc7-3f9c64bbea2e",
   "metadata": {
    "height": 62
   },
   "outputs": [],
   "source": [
    "with open(\"files/llama-models_files.txt\", \"r\", encoding='utf=8') as file:\n",
    "    repo = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9365464e-7d56-4ba3-bb0c-54c951a9c6e7",
   "metadata": {
    "height": 113,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find which file has the function `_encode_image` defined, we need to search through the provided content.\n",
      "\n",
      "The function `_encode_image` is defined in the file:\n",
      "```\n",
      "Path: ./llama-models/llama-models-main/models/llama4/chat_format.py\n",
      "```\n",
      "This function is part of the `ChatFormat` class and is used to encode an image into a list of token IDs. \n",
      "\n",
      "Here's the specific part of the code where the function is defined:\n",
      "\n",
      "```python\n",
      "def _encode_image(\n",
      "    self,\n",
      "    transformed_image: TransformedImage,\n",
      ") -> List[int]:\n",
      "    assert self.vision_args is not None, \"The model is not vision-enabled\"\n",
      "\n",
      "    image_tensor = transformed_image.image_tiles\n",
      "    image_channels = image_tensor.shape[-3]\n",
      "    image_height = image_tensor.shape[-2]\n",
      "    image_width = image_tensor.shape[-1]\n",
      "    image_chunks = image_tensor.view(-1, image_channels, image_height, image_width).shape[0]\n",
      "\n",
      "    patch_height = self.vision_args.patch_size.height\n",
      "    patch_width = self.vision_args.patch_size.width\n",
      "\n",
      "    if image_height % patch_height != 0:\n",
      "        raise ValueError(f\"{image_height=} not divisible by {patch_height=}\")\n",
      "    if image_width % patch_width != 0:\n",
      "        raise ValueError(f\"{image_width=} not divisible by {patch_width=}\")\n",
      "\n",
      "    ds_ratio = int(round(1.0 / (self.vision_args.pixel_shuffle_ratio**2)))\n",
      "    n_patches_per_chunk = int((image_height // patch_height) * (image_width // patch_width) // ds_ratio)\n",
      "\n",
      "    image_ar = transformed_image.aspect_ratio\n",
      "    tokens = [self.tokenizer.special_tokens[\"<!--\"]]\n",
      "    if image_chunks == 1:\n",
      "        tokens += [self.tokenizer.special_tokens[\"<s>\"]]\n",
      "        tokens += [self.tokenizer.special_tokens[\"<image>\"]] * n_patches_per_chunk\n",
      "        tokens += [self.tokenizer.special_tokens[\"</s>\"]]\n",
      "    else:\n",
      "        ratio_h, ratio_w = image_ar\n",
      "        for _ in range(ratio_h):\n",
      "            for xx in range(ratio_w):\n",
      "                tokens += [self.tokenizer.special_tokens[\"<image>\"]] * n_patches_per_chunk\n",
      "                if xx < ratio_w - 1:\n",
      "                    tokens.append(self.tokenizer.special_tokens[\"<-->\"])\n",
      "\n",
      "                tokens.append(self.tokenizer.special_tokens[\"</-->\"])\n",
      "\n",
      "        tokens += [self.tokenizer.special_tokens[\"<s>\"]]\n",
      "        tokens += [self.tokenizer.special_tokens[\"<image>\"]] * n_patches_per_chunk\n",
      "        tokens += [self.tokenizer.special_tokens[\"</s>\"]]\n",
      "\n",
      "    return tokens\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Which file in the content below, which consists of a list\n",
    "of source file path and its content, has the function _encode_image\n",
    "defined? content:\n",
    "{repo}\"\"\"\n",
    "print(llama4_together(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c976d3b-f6db-493f-a705-646692ebccfe",
   "metadata": {},
   "source": [
    "## Analyze extensive user activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efe1bf0d-4b34-4b18-adc6-0bf5ab01967b",
   "metadata": {
    "height": 351
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962 Fix README links for use cases\n",
      "959 Created an api inference script with its supporting documentation\n",
      "958 add vertex notebooks for gcp\n",
      "957 Updated notebook to use Llama API\n",
      "956 Bootcamp week2 task\n",
      "954 Adding Databricks RAG example recipe\n",
      "953 Fix deprecated langchain warning for llama3\n",
      "951 Fix model string name for Llama-Prompt-Guard-2-86M in inference.py\n",
      "950 Fix/update samsum\n",
      "944 [Llama Tools] Getting started fixes for llama-prompt-ops\n",
      "943 fix failing github actions\n",
      "942 Llama 4 Fine tuning: updated docs to streamline environment setup and clarify command line arguments\n",
      "941 [Llama Tools] Getting started guide for llama-prompt-ops \n",
      "940 Fix/GitHub action runner\n",
      "939 fix links and references\n",
      "937 Notebook for generating evals using sythetic data\n",
      "935 [Hotfix] Pytest Workflow AndroidManifest.xml issue\n",
      "932 Llama4 Fine-tuning using torchtune\n",
      "931 [WIP] image grounding example\n",
      "930 fixing readme links\n",
      "929 Update README.md\n",
      "928 Llama 4 api recipes\n",
      "927 Llama 4 api release\n",
      "926 Update android app\n",
      "925 changed planning prompt, execution prompt, few shot examples, and bro‚Ä¶\n",
      "924 changed planning prompt, execution prompt, few shot examples, and bro‚Ä¶\n",
      "923 updated browser navigation use case readme\n",
      "922 updated recipe to use llama 4\n",
      "921 Removed redundant 'FAQ' in README.md and corrected typo\n",
      "919 E2E use-case:  research paper analyzer with Llama 4\n",
      "918 add ms-swift & llama4 SFT recipe\n",
      "917 Update README.md\n",
      "916 [Llama4] Book character mindmap\n",
      "915 Add Android Article Summarizer demo app\n",
      "914 Make docs link version agnostic on README\n",
      "913 Update build_with_llama_4.ipynb\n",
      "912 Fix Maverick typo\n",
      "911 push link fix\n",
      "910 Llama 4 release \n",
      "907 Update `hello_llama_cloud.ipynb` source\n",
      "906 Google Calendar Assistant with with Llama 3.2 3B  Tool Calling \n",
      "905 Clean / update LangGraph Tutorial\n",
      "902 update HELM link\n",
      "900 Bump tj-actions/changed-files from 41.0.0 to 45.0.8 in /.github/workflows\n",
      "898 Notebook showing how to fine tune llama guard with torchtune\n",
      "897 Add SalesBot Weaviate Notebook\n",
      "896 Rename README.md to Off\n",
      "895 format markdown faq\n",
      "892 update test for guard prefix\n",
      "891 fix sharding strategy str to enum conversion\n",
      "890 update category prefix according to docs\n",
      "889 Contextual keywords generation for RAG using Llama-3.1\n",
      "888 Update tool calling nbs to 3.3\n",
      "887 Update Step-1 PDF-Pre-Processing-Logic.ipynb\n",
      "886 add colab instruction, fix links\n",
      "885 [WIP] update Tool calling 101 from 3.1 to 3.3\n",
      "883 remove key\n",
      "882 ignore medium and linkedin\n",
      "881 added readme for customerservice chatbots\n",
      "880 update root README with absolute path to fix https://pypi.org/project/llama-cookbook/ broken links\n",
      "879 added readme to broswer use usecase\n",
      "878 Fix of NameError(missed fucntion call)\n",
      "877 fix link path\n",
      "875 added readme to text2sql end to end usecase\n",
      "873 deprecate OctoAI\n",
      "871 Fix/load model with torch dtype auto #663 after cookbook refactor\n",
      "869 fix notebook\n",
      "867 Ignore HTTP response 429 during link check\n",
      "866 add MIT license to root\n",
      "865 Moving responsible ai to getting_started\n",
      "864 Fix more package naming\n",
      "862 fix meta_eval after refactor and add new meta_mmlu_instruct task for 3.2\n",
      "861 Fix package naming\n",
      "860 Fixed notebookllama path readme\n",
      "859 Fix typo\n",
      "858 Fixing of typo in Step-2-Transcript-Writer.ipynb\n",
      "857 Typo fix in Step-2-Transcript-Writer.ipynb\n",
      "856 Fix: Updated broken documentations links and references\n",
      "853 fix few typos\n",
      "852 Fix package naming\n",
      "851 Cleaned up API KEY placeholder text\n",
      "850 Fixed all \"Open in Colab\" absolute paths\n",
      "848 update branch\n",
      "847 MM-RAG-New-PR\n",
      "846 [DRAFT] Fix/refactor recipe references\n",
      "844 fix links cookbook refactor\n",
      "843 Update README.md\n",
      "842 Add FAQ\n",
      "841 rebase\n",
      "840 fix double bos for vision model\n",
      "838 Fix a property name of list_emails function\n",
      "834 fix/fix few typos\n",
      "833 Update wordlist.txt\n",
      "832 Refactor Llama-Recipes\n",
      "829 update parsing of dataset_config.file to prevent custom-function-name from clobbering data-collator name. \n",
      "827 Typo in prompt_format_utils.py\n",
      "825 Llama code/code review agents\n",
      "823 moved images from quickstart to doc/img\n",
      "822 update text2sql demo to using Llama 3.3\n",
      "808 [DRAFT] prompt migration engine\n",
      "807 fix link\n",
      "806 Typo in Prompt_Engineering_with_Llama_3.ipynb \n",
      "805 Llama Email Agent\n",
      "803 fix links\n",
      "801 Add llama 3.2 mmlu, math, gpqa evals to meta_eval harness\n",
      "799 Browser use with Llama 3.2 Vision Quickstart\n",
      "798 add 5 together llama notebooks\n",
      "797 [WIP] Add FSDP2 \n",
      "796 [docs] small typo in eval readme\n",
      "794 rebase\n",
      "793 fix typo in auto wrap policy\n",
      "791 add freeze_LLM_only option for mllama finetuning\n",
      "790 colab links fix\n",
      "789 Rebase\n",
      "787 fix walkthrough.ipynb render\n",
      "786 [draft] [bug] Fix ipynb rendering\n",
      "778 default to trusted_code for Samsum dataset\n",
      "775 Update samsum_dataset.py\n",
      "772 Update wandb.py to accept setting run name from command line argument (e.g., --wandb_config.name \"run_name\") for fine tuning\n",
      "769 Update Step-1 PDF-Pre-Processing-Logic.ipynb to remove \"text not defined\"\n",
      "768 Add E2B AI Analyst\n",
      "767 Updated QuickStart README\n",
      "766 add TogetherNotebookLM recipe\n",
      "764 added a notebook tutorial on llama 3.2 new capabilities\n",
      "763 Fix quickstart NotebookLlama's dependencies\n",
      "761 Fix NotebookLlama step 4 dependencies\n",
      "760 1. fixed \"INPUT_FILE\" path   2. fix create_word_bounded_chunks before read in the text variables  3) processed_text not definedPatch 1\n",
      "759 fix \"INPUT_FILE\" path and missing variable \"processed_text\"\n",
      "758 Update Step-1 PDF-Pre-Processing-Logic.ipynb\n",
      "757 All functionality has been consolidated into a single file for CLI/UI/Checkpointing and Added fix for issue 702 and added code for that as well, added instructions in local_inference README.md\n",
      "756 Update Step-1 PDF-Pre-Processing-Logic.ipynb\n",
      "755 Quickstart docs: Fix path to location of dict for custom datasets \n",
      "754 Update hello_llama_cloud.ipynb\n",
      "753 Update hello_llama_cloud.ipynb\n",
      "751 Add files via upload\n",
      "750 add support for ingesting content from websites, audio files, YouTube, etc.\n",
      "748 Fix minor grammatical errors\n",
      "746 Small notes on next steps\n",
      "745 Update wordlist.txt\n",
      "744 Append epoch rather than best val. loss to val_loss\n",
      "743 Create SECURITY.md\n",
      "742 add llama3 support for alpaca dataset\n",
      "741 Save the `preprocessor_config.json` and `chat_template.json` for mllama model after conversion\n",
      "739 Notebook llama\n",
      "737 Support converting fine-tuned llama 3.2 vision model to HF format and then local inference\n",
      "736 Update wordlist.txt\n",
      "733 Implement test loss\n",
      "731 Zero-to-Llama-Course\n",
      "729 quick fix on readmes and deadlinks\n",
      "728 Fix numpy seed in finetuning.py\n",
      "726 Fix/unit test 3.2\n",
      "725 Fix fine-tuning training loss accumulation\n",
      "723 Llama 3.2\n",
      "720 fix Colab link in quickstart_peft_finetuning.ipynb\n",
      "719 Fix link to LLM finetuning overview\n",
      "718 Added a Gradio UI for multi-modal inferencing using Llama 3.2 Vision/\n",
      "717 Fix the bug when continue the peft.\n",
      "716 Initial Crusoe examples to 3p_integrations recipes\n",
      "712 Llama 3.2 vision\n",
      "708 add support for llama vision model conversion\n",
      "707 added missing word and corrected spelling\n",
      "706 Adds Llama 3.2 example on Modal with a fun experiment\n",
      "705 added instructions in README.md for proper multi modal inferencing with Llama 3.2 models\n",
      "704 [Fixed] RuntimeError: probability tensor contains either inf, nan or element < 0\n",
      "703 fix raft dataset\n",
      "700 Delete cookie\n",
      "698 Multi modal rag demo\n",
      "697 Tool Calling Tutorial and Example\n",
      "696 Update multi_modal_infer.py\n",
      "691 Improve model checkpoint saving logic\n",
      "687 post1 release version bump\n",
      "686 fix AutoModel and bump transformers version to 4.45\n",
      "685 Requirements.txt version bump\n",
      "684 Improve discoverability of 3.2 recipes\n",
      "681 update mutligpu readme and MllamaForConditionalGeneration import\n",
      "679 fix readme\n",
      "678 Create v0.0.4 release\n",
      "677 Upstream merge\n",
      "676 Make gradio and langchain optional dependencies\n",
      "672 Adding data prep recipes from data-prep-kit\n",
      "667 LG notebook - Repair broken import and add note about dependency\n",
      "666 [LG Notebook] Adjust category removal example\n",
      "665 Fix/custom dataset chat template\n",
      "664 Update requirements.txt\n",
      "663 Fix/load model with torch dtype auto\n",
      "662 Update get_default_finetune_args.py\n",
      "660 Enhance script to handle all text file extensions\n",
      "659 Adding custom dataset file\n",
      "657 add_rocm_support_for_mixed_precision\n",
      "654 Model saving w/o FSDP or PEFT\n",
      "653 fix that help reaching 50% over binary classification of toxic chat\n",
      "651 Add recipe for Llama Triaging & Reporting Tool\n",
      "650 Fix checkpoint saving \n",
      "643 Fix version number in Python example\n",
      "639 Updates to accommodate OpenLLM leaderboard v2 tasks and change Meta Llama 3.1 to Llama 3.1 \n",
      "638 Update hello_llama_cloud.ipynb\n",
      "636 Add preprocessor to patch PromptGuard scores for inserted characters\n",
      "635 recipes/quickstart/Getting_to_know_Llama.ipynb,  typo fix lama -> llama line 127\n",
      "632 Updating llama 3 references to 3.1 model\n",
      "630 SFT attempt\n",
      "629 Use new get_model_state_dict api for save_pretrained peft model\n",
      "628 Enable users to trust remote code in samsum dataset\n",
      "627 Eval reproduce recipe using lm-evaluation-harness and our 3.1 evals datasets\n",
      "622 Update checkpoint_converter_fsdp_hf.py\n",
      "619 Llamaguard notebook colab link fix\n",
      "618 [Recipe] Example featuring built-in tool calling capabilities - Wolfram Alpha, Interpreter, Brave Search\n",
      "616 Move supported features table to main README\n",
      "615 [Azure] Update Azure API usage example to 3.1\n",
      "614 Update readme text to be version-agnostic\n",
      "609 Fill in one sentence in the prompt guard tutorial.\n",
      "608 Add PromptGuard to safety_utils\n",
      "606 Address feedback not possible before launch in LG3 recipe and dataset file\n",
      "605 Update transformers requirements\n",
      "604 Remove max_length from tokenization\n",
      "603 Release update\n",
      "602 Corrected wrong order of commands\n",
      "601 Move MediaGen notebook to octoai folder\n",
      "597 Fix broken image link\n",
      "596 Fix relative links to images\n",
      "594 Port of DLAI LlamaIndex Agent short course lessons 2-4 to use Llama 3\n",
      "593 colab links fixed for dlai agents notebooks\n",
      "591 fix llama3 cookbook with llamaindex and groq\n",
      "590 Updating chatbot folder names\n",
      "589 Update links in README.md\n",
      "588 fix typo\n",
      "587 changed --pure_bf16 to --fsdp_config.pure_bf16 and corrected \"examples/\" path\n",
      "586 Update 3p_integration README.md\n",
      "585 Add experimental folder to README\n",
      "584 Update hello_llama_cloud.ipynb\n",
      "583 Fix broken images in LLM finetuning overview\n",
      "582 Deleting Agents folder and adding llamaindex\n",
      "581 Updating the folder name 3p_integrations\n",
      "580 Updated the folder name 3p_integrations\n",
      "578 Add README for quickstart + update to codellama url\n",
      "577 Updates to benchmarks code\n",
      "576 Add Langchain agent notebooks to 3P_Integrations\n",
      "575 New structure and rename for tools, docs and quickstart folder\n",
      "573 [lamini] Add lamini text2sql memory tuning tutorial\n",
      "572 Adding support for FSDP+Qlora.\n",
      "570 bug fix\n",
      "569 Adding end-to-end llama chatbot recipe using Retrieval Augmented Fine Tuning (RAFT)\n",
      "568 RAFT fine-tuning technique with Deep Lake Dataloader\n",
      "563 None\n",
      "562 Make quickstart finetuning notebook ready for T4\n",
      "560 4 notebooks ported from 4 DLAI agent short courses using Llama 3\n",
      "559 Add ToolMessage import\n",
      "558 [WIP] Peft Finetuning Quickstart Notebook\n",
      "557 add multiturn-conversation dataset process\n",
      "555 Minor update to README\n",
      "554 Adding Torchtune recipe for fine-tuning\n",
      "553 Add Groq/Llama3 recipes (cookbook and command line examples)\n",
      "552 Add groq cookbook entries using Llama3\n",
      "551 Update hf weight conversion script to llama 3\n",
      "550      Initial work on customizing LG\n",
      "549 Update langgraph tool calling agent, simplify examples and README\n",
      "547 Remove pkg_resources.packaging\n",
      "546 replace groq llama 2 with replicate\n",
      "545 Fix typo in Getting_to_know_Llama.ipynb\n",
      "544 Correct the correct url\n",
      "542 add multiturn-conversation dataset process\n",
      "540 fixed alpaca dataset evalset length and make sure len(eval_loader)>0\n",
      "532 Fix config file links for FMBench, update business summary chart.\n",
      "531 Resume the fine-tuning process from the previous PEFT checkpoint folder\n",
      "529 bump up version\n",
      "528 Update contribution.md\n",
      "527 Updating links to running llama3 locally\n",
      "526 Move tests and scripts into subfolders\n",
      "525 Bump tj-actions/changed-files from 29.0.4 to 41.0.0 in /.github/workflows\n",
      "524 Updated model names for OctoAI\n",
      "523 fix singlegpu_finetuning.md typos\n",
      "522 document less obvious training config parameters\n",
      "516 Freeze layer bug fix\n",
      "515 Create Prompt_Engineering_with_Llama_3_On_Amazon_Bedrock.ipynb\n",
      "514 Remove local tokenizer requirement for vllm on prem throughput benchmark\n",
      "513 Update langgraph agent recipe\n",
      "512 Update azure_api_example.ipynb for Llama 3 context\n",
      "510 update readme and delete stale content\n",
      "507 Fixing path issues\n",
      "506 [MLC-LLM] Introducing Llama 3 running locally on Android using MLC-LLM\n",
      "503 add llamaindex + groq cookbook\n",
      "502 add llamaindex cookbook\n",
      "501 Fixing lib installation in notebook example\n",
      "500 [OctoAI] Llama-3 based summarization + RAG to power a sales bot\n",
      "499 Fix save metric FileNotFoundError when finetuning\n",
      "498 Fix save metric FileNotFoundError when finetuning\n",
      "497 Fix save metric FileNotFoundError when finetuning\n",
      "496 [OctoAI] Introducing a Video Generation use case featuring Llama3\n",
      "495 Prompt script and notebook for easy prompt formatting and function definition\n",
      "494 [OctoAI model provider] Llama3 update\n",
      "493 Modify langgraph agent to use same tools as tool-use-agent\n",
      "492 changed readme.md and parameters.json to support llama3 vllm benchmark\n",
      "488 Fix lm_eval.tasks' has no attribute 'initialize_tasks' error\n",
      "487 Fix param_init_fn: move if-statement out of lambda\n",
      "486 FMBench readme updates for Llama3 on Inf2 and config.yml cleanup\n",
      "485 Add LangChain recipes\n",
      "484 Update prompt guide for Llama 3\n",
      "483 use AutoTokenizer instead of LlamaTokenizer in checkpoint_converter_fsdp_hf.py\n",
      "482 Disable prefix tuning and limit llama adapter\n",
      "481 add transformerengine support\n",
      "480 Finetuning Llama2 7b by QLoRA 4bit\n",
      "479 Updated fine-tuning readme to Meta Llama 3\n",
      "478 Changing the text in the azure notebook from Llama 2 to Llama.\n",
      "477 L3updates/octoai\n",
      "474 Update aws notebooks\n",
      "473 Update Chatbot recipe with Llama 3\n",
      "471 [WIP]adding fsdp-qlora in progress\n",
      "469 folder name change for llama3\n",
      "468 updating StructuredLlama, Messenger and WhatsApp for Llama 3\n",
      "467 update VideoSummary, LiveData, llama-on-prem for Llama 3\n",
      "466 Removed notebook output cells\n",
      "465 Llama3update\n",
      "464 Llama3update\n",
      "463 Llama3update\n",
      "462 [WIP] adding chatbot-e2e\n",
      "461 Update peft_finetuning.ipynb\n",
      "460 Dev\n",
      "459 updated Running_Llama_on_Mac and HelloLlamaCloud.ipynb for Llama 3 \n",
      "458 Update the Getting to know Llama notebook for Llama 3 (with comparison to Llama 2).\n",
      "457 Update Getting_to_know_Llama.ipynb\n",
      "456 updating the READMEs to llama3\n",
      "455 Q&A RAG pipeline with MongoDB, Hugging Face and Llama3\n",
      "452 FMBench: benchmarking Llama models on AWS\n",
      "451 Create Karimoz\n",
      "448 Update llama_guard_version in inference.py\n",
      "447 [Feature]Enable Ascend NPU fintuning and inference\n",
      "446 Update xpu related device setting\n",
      "443 Fix llama3 urls + chat completion termination + nightlies in readme\n",
      "441 bumping transformer versions for llama3 support\n",
      "440 L3p/readme prompt updates\n",
      "439 Updates for Llama 3\n",
      "437 Add a README to provide links to API providers\n",
      "434 Bump gradio from 4.16.0 to 4.19.2 in /recipes/llama_api_providers/OctoAI_API_examples/RAG_Chatbot_example\n",
      "433 Added a feature that allow users to use pytorch profiler or flop_counter to measure the performance during fine-tuning.\n",
      "432 Changes for aegis fine-tuning\n",
      "431 Llama2 on EC2 tutorial using Runhouse.\n",
      "429 Recipe to add a new language to Llama2\n",
      "428 Adding a feature that will stop the training/eval process after reaching some max_steps\n",
      "427 fix: assigned correct path to custom dataset example file\n",
      "419 Add note on CUDA version + remove 'test' from pytorch whl url\n",
      "418 Remove openai from example notebook and llm.py class. Simplify notebook layout\n",
      "417 Update location and name of llm.py example notebook\n",
      "415 only save training params on rank 0\n",
      "411 Implement H2O for long context inference on summarization tasks\n",
      "407 update due to peft new release\n",
      "404 Reorg inference throughput folder structure\n",
      "403 [WIP/Do Not Review]Chatbot Recipe\n",
      "402 Fix hsdp_device_mesh=None when enable HSDP and HYBRID_SHARD\n",
      "400 Adding Llama Guard notebooks\n",
      "399 Reorg Benchmark folder structure\n",
      "398 Add llm class so that externally-hosted models can be called\n",
      "397 Fix dead links after directory structure refactor\n",
      "395 Adding open in colab option for notebook\n",
      "394 Refactor the folder structure to organize recipes by topic\n",
      "392 Fix checkpoint converter to use arguments before default behavior of checking yaml\n",
      "391 Refactor the folder structure to organize recipes by topic\n",
      "390 removing legacy code for sdpa\n",
      "386 Updating the AWS Prompt_Engineering Notebook + Adding an Example of ReAct with Llama 2 on Bedrock\n",
      "384 Add gradio to requirements.txt\n",
      "379 [Demos] Adding OctoAI API Examples (Hosted Llama solution)\n",
      "377 Fix `load_model` missing argument\n",
      "376 Adding examples and updating existing examples using Amazon Bedrock\n",
      "375 Add Llama2 + EC2 tutorial using Runhouse\n",
      "373 forking repo\n",
      "372 Fix/missing copyright headers\n",
      "371 Update README.md adding setuptools installation for Mac\n",
      "370 Make tests run on cpu only machines\n",
      "369 Introducing GH Actions workflow to run llama-recipes PyTest tests\n",
      "367 Add gradio library for user interface in inference.py\n",
      "365 Codellama 70b instruct code example\n",
      "363 Remove deprecated pytest_cmdline_preparse\n",
      "362 Fix Anyscale API token URL in Purple_Llama_Anyscale.ipynb\n",
      "361 Revert \"Flop counter, profiling and GC (#357)\"\n",
      "357 Flop counter, profiling and GC\n",
      "356 Eval harness \n",
      "355 Fix broken format in preview for RAG chatbot example\n",
      "354 Add option to enable Llamaguard content safety check in chat_completion\n",
      "353 Add Prompt Engineering with Llama 2\n",
      "351 Update README.md\n",
      "350 fix of dead link in demo apps readme\n",
      "346 typo fix in Purple_Llama_Anyscale.ipynb\n",
      "343 Adding test cases for the bugs found, local_rank None and output dir ‚Ä¶\n",
      "342 Update Installation section of README.md\n",
      "339 Add script to run cloud api throughput benchmark\n",
      "337 Llama guard data formatter example\n",
      "336 Tanner sorensen patch 1\n",
      "334 Fix some small nits in README.md\n",
      "331 Add inference throughput benchmark on-prem vllm\n",
      "327 Fix test_finetuning for env without cuda\n",
      "326 Create finetuning data formatter for llama-guard\n",
      "325 Changing Llama Guard safety check to HF classes\n",
      "324 Add examples for Azure Llama 2 API (Model-as-a-Service)\n",
      "323 Purple llama Anyscale\n",
      "322 Adding script to execute Llama Guard standalone\n",
      "321 Removing unecesary prompts\n",
      "319 Add example conversion script to convert hf to consolidated weight format\n",
      "318 Fix/unit tests\n",
      "317 remove additional space in llama-guard category\n",
      "316 Fixing committed files\n",
      "315 Feature/llamaguard safety checker integration\n",
      "314 Adding Llama Guard safety checker.\n",
      "313 Update wordlist.txt for additional technical terms in Readme.\n",
      "311 Messenger Llama2 Chatbot Tutorial\n",
      "308 Update grammar_dataset_process.ipynb \n",
      "307 Update README.md\n",
      "306 PR based on the chester-rag-chatbot-example branch\n",
      "305 Fix a typo in the README.md of demo apps\n",
      "304 Adding W&B (wandb) experiment tracking\n",
      "302 Support CodeLlama and 70B training (CPU offload on save)\n",
      "291 Add complete Chatbot example with RAG capability for demo apps\n",
      "287 A tutorial of building a Llama-enabled WhatsApp Chatbot\n",
      "283 Use bf16 parameters in bf16 mixed prec\n",
      "281 adding HSDP as sharding strategy for FSDP training composable with PEFT\n",
      "280 Add gradient clipping feature\n",
      "279 Llama 2 On-Prem Inference Using vLLM and TGI\n",
      "272 Fix examnples.py filename error\n",
      "271 Update README.md\n",
      "269 Fix typo\n",
      "264 Typo Correction: Update HelloLlamaLocal.ipynb\n",
      "260 Update HelloLlamaCloud.ipynb\n",
      "259 added pypdf to !pip install \n",
      "258 Custom Changes\n",
      "254 Adding context to the demo notebooks\n",
      "251 fix incorrect split of InstructionDataset\n",
      "250 Introduce a tracker API and integrate Aimstack tracker\n",
      "249 Introducing Actions CI/CD Pytest workflow\n",
      "243 Llama2 demo apps - 6 notebooks to run Llama2, with llama-cpp, LangChain and LlamaIndex integration\n",
      "237 Create codeql.yml\n",
      "230 Add missing amp context if use_fp16 is enabled\n",
      "229 Add Flop counter & proifler\n",
      "228 minor updates\n",
      "227 Llama 2 Connect 2023 Colab Notebook\n",
      "224 Updated quickstart notebook to use llama_recipes package\n",
      "220 Feature/plotting\n",
      "217 Introducing Github Actions CI/CD workflow to run pytests on self-hosted runners in AWS.\n",
      "216 Fix truncation of training examples in alpaca dataset\n",
      "212 Make tests run on cpu instance\n",
      "211 Invalidate labels in dialog dataset to disable loss\n",
      "207 adding chat special tokens to tokenizer\n",
      "206 Feature/length based batch sampling\n",
      "204 adding system prompt to custom dataset\n",
      "203 adding optimizer overlap for FSDP\n",
      "201 Fix tqdm bar not change length after terminal is resized\n",
      "200 [WIP] feat: add wandb\n",
      "197 pass weight_decay into optimizer\n",
      "196 Fix vocab size mismatch in inference due to added pad token\n",
      "185 Fix typo in chat_completion.py\n",
      "183 bugfix: update tqdm bar with the fixed gradient_accumulation_steps\n",
      "180 Use OpenAssistent/oasst1 dataset for custom dataset example\n",
      "178 Allow easier use of custom datasets\n",
      "177 Add missing license header in test files\n",
      "176 Set release version to 0.0.1\n",
      "165 Pass `use_cache=False` when training with FSDP\n",
      "162 Wandb, cpu offloading, cosine scheduler, CodeLlama, Completion and bug fixes\n",
      "158 Fix the chat example for generate when load peft model\n",
      "157 [WIP] prep sagemaker settings (dont review pls)\n",
      "153 Package distribution\n",
      "152 Remove micro_batch_training parameter \n",
      "151 adding notes how to get the HF models\n",
      "149 Update LLM_finetuning.md\n",
      "147 LoRa Config parameter flowthrough fix\n",
      "144 adding llama code inference\n",
      "143 pulled changes from fb main branch\n",
      "136 Update inference.md\n",
      "131 remove duplicate peft model load\n",
      "128 Fix \"load_model_sharded\" when \"--save_optimizer\" enabled\n",
      "125 The tokenizer will not add eos_token by default\n",
      "124 bugfix: remove duplicate load_peft_model\n",
      "122 Add FSDP CPU offloading option\n",
      "121 fix a bug in the config for use_fast_kernels\n",
      "116 Feature : Enable Intel GPU/XPU finetuning and inference\n",
      "115 Fix & improve activation offloading\n",
      "112 enable grad on loss tensor\n",
      "110 Add sample inference script for hf-tgi\n",
      "107 Fix a bug in alapaca dataset, set ignore_idx=-100.\n",
      "106 Fix finetuning batch size\n",
      "105 update prompts\n",
      "104 Chat prompt fix\n",
      "98 fix some typos in downloading alpaca_dataset\n",
      "97 adding flash attention and xformer memory efficient through PT SDPA\n",
      "94 Fix filename typo\n",
      "93 fix some typos.\n",
      "88 Update spellcheck.sh\n",
      "85 Update paddings \n",
      "84 Coga\n",
      "77 save cpu mem by leveraging FSDP rank0 broadcasting\n",
      "75 deleted\n",
      "70 Use `defaultdict` to avoid hard coding buffer keys in `Concatenator` and `ConcatDataset`\n",
      "67 Templates updates\n",
      "63 update README: python 3.9 rec + fix formatting\n",
      "57 adding issue tempalte\n",
      "51 fixing the full state path in checkpoint handler+loss report calculation\n",
      "50 Create spellcheck.yml\n",
      "49 Fix broken links in Dataset.md\n",
      "44 adding active mem stat\n",
      "41 Improve FSDP LoRA Memory Usage\n",
      "40 Fix cuda id for using quantization\n",
      "39 Fsdp inference checkpoints\n",
      "34 fixing scaler for both fsdp and non fsdp\n",
      "33 fixing the condition for moving to cuda\n",
      "31 Update README.md\n",
      "28 modify to steping the lr scheduler each epoch\n",
      "26 Adding Supporting Files For link and Spell Check\n",
      "24 fix save full state dict checkpoint\n",
      "20 fix typos and spelling errors\n",
      "19 Update README.md\n",
      "12 Inference updates\n",
      "11 docs: fix typo in README.md\n",
      "10 Fixed Typo in README.md\n",
      "9 Update grammar_dataset_process.ipynb\n",
      "8 Update inference.md\n",
      "4 Doc requirements update\n",
      "1 remove unneeded imports for llama_finetuning.py\n"
     ]
    }
   ],
   "source": [
    "from utils import get_pull_requests\n",
    "import pickle\n",
    "repo_owner = \"meta-llama\"\n",
    "repo_name = \"llama-cookbook\"\n",
    "\n",
    "# We have saved the pull_requests as a pickle file to make this run faster by loading files from local.\n",
    "cache_file = \"meta-llama_llama-cookbook_pull_requests.pkl\"\n",
    "load_cache_file = True\n",
    "\n",
    "if load_cache_file:\n",
    "    with open(cache_file, \"rb\") as f:\n",
    "        pull_requests = pickle.load(f)\n",
    "else:\n",
    "    pull_requests = get_pull_requests(repo_owner, repo_name)\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(pull_requests, f)\n",
    "\n",
    "for pr in pull_requests:\n",
    "    print(pr[\"number\"], pr[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "326a0642-5239-421d-ac64-042a0e13666d",
   "metadata": {
    "height": 79
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(pull_requests, open(f\"{repo_owner}_{repo_name}_pull_requests.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4db33e4a-ffa8-42f6-a078-0867a945c723",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pull_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56937fce-0f3c-4812-a7d8-9f8fd4aa9244",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://api.github.com/repos/meta-llama/llama-cookbook/pulls/962',\n",
       " 'id': 2584943236,\n",
       " 'node_id': 'PR_kwDOJ8YnuM6aExqE',\n",
       " 'html_url': 'https://github.com/meta-llama/llama-cookbook/pull/962',\n",
       " 'diff_url': 'https://github.com/meta-llama/llama-cookbook/pull/962.diff',\n",
       " 'patch_url': 'https://github.com/meta-llama/llama-cookbook/pull/962.patch',\n",
       " 'issue_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/issues/962',\n",
       " 'number': 962,\n",
       " 'state': 'closed',\n",
       " 'locked': False,\n",
       " 'title': 'Fix README links for use cases',\n",
       " 'user': {'login': 'fbnav',\n",
       "  'id': 82674371,\n",
       "  'node_id': 'MDQ6VXNlcjgyNjc0Mzcx',\n",
       "  'avatar_url': 'https://avatars.githubusercontent.com/u/82674371?v=4',\n",
       "  'gravatar_id': '',\n",
       "  'url': 'https://api.github.com/users/fbnav',\n",
       "  'html_url': 'https://github.com/fbnav',\n",
       "  'followers_url': 'https://api.github.com/users/fbnav/followers',\n",
       "  'following_url': 'https://api.github.com/users/fbnav/following{/other_user}',\n",
       "  'gists_url': 'https://api.github.com/users/fbnav/gists{/gist_id}',\n",
       "  'starred_url': 'https://api.github.com/users/fbnav/starred{/owner}{/repo}',\n",
       "  'subscriptions_url': 'https://api.github.com/users/fbnav/subscriptions',\n",
       "  'organizations_url': 'https://api.github.com/users/fbnav/orgs',\n",
       "  'repos_url': 'https://api.github.com/users/fbnav/repos',\n",
       "  'events_url': 'https://api.github.com/users/fbnav/events{/privacy}',\n",
       "  'received_events_url': 'https://api.github.com/users/fbnav/received_events',\n",
       "  'type': 'User',\n",
       "  'user_view_type': 'public',\n",
       "  'site_admin': False},\n",
       " 'body': '# What does this PR do?\\r\\n\\r\\nFixes links for the use cases in the README\\r\\n',\n",
       " 'created_at': '2025-06-11T17:22:13Z',\n",
       " 'updated_at': '2025-06-11T17:31:59Z',\n",
       " 'closed_at': '2025-06-11T17:31:54Z',\n",
       " 'merged_at': '2025-06-11T17:31:54Z',\n",
       " 'merge_commit_sha': '0f4e671f2ccb794b6017391d10a87f0f2af8a2c9',\n",
       " 'assignee': None,\n",
       " 'assignees': [],\n",
       " 'requested_reviewers': [{'login': 'carljparker',\n",
       "   'id': 1108664,\n",
       "   'node_id': 'MDQ6VXNlcjExMDg2NjQ=',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/1108664?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/carljparker',\n",
       "   'html_url': 'https://github.com/carljparker',\n",
       "   'followers_url': 'https://api.github.com/users/carljparker/followers',\n",
       "   'following_url': 'https://api.github.com/users/carljparker/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/carljparker/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/carljparker/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/carljparker/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/carljparker/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/carljparker/repos',\n",
       "   'events_url': 'https://api.github.com/users/carljparker/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/carljparker/received_events',\n",
       "   'type': 'User',\n",
       "   'user_view_type': 'public',\n",
       "   'site_admin': False}],\n",
       " 'requested_teams': [],\n",
       " 'labels': [{'id': 5749763608,\n",
       "   'node_id': 'LA_kwDOJ8YnuM8AAAABVrZuGA',\n",
       "   'url': 'https://api.github.com/repos/meta-llama/llama-cookbook/labels/cla%20signed',\n",
       "   'name': 'cla signed',\n",
       "   'color': 'ededed',\n",
       "   'default': False,\n",
       "   'description': None}],\n",
       " 'milestone': None,\n",
       " 'draft': False,\n",
       " 'commits_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/pulls/962/commits',\n",
       " 'review_comments_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/pulls/962/comments',\n",
       " 'review_comment_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/pulls/comments{/number}',\n",
       " 'comments_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/issues/962/comments',\n",
       " 'statuses_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/statuses/6477af074bb1724bd42d14831f341d7525e63b4c',\n",
       " 'head': {'label': 'meta-llama:fix-links',\n",
       "  'ref': 'fix-links',\n",
       "  'sha': '6477af074bb1724bd42d14831f341d7525e63b4c',\n",
       "  'user': {'login': 'meta-llama',\n",
       "   'id': 153379578,\n",
       "   'node_id': 'O_kgDOCSRi-g',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/153379578?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/meta-llama',\n",
       "   'html_url': 'https://github.com/meta-llama',\n",
       "   'followers_url': 'https://api.github.com/users/meta-llama/followers',\n",
       "   'following_url': 'https://api.github.com/users/meta-llama/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/meta-llama/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/meta-llama/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/meta-llama/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/meta-llama/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/meta-llama/repos',\n",
       "   'events_url': 'https://api.github.com/users/meta-llama/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/meta-llama/received_events',\n",
       "   'type': 'Organization',\n",
       "   'user_view_type': 'public',\n",
       "   'site_admin': False},\n",
       "  'repo': {'id': 667297720,\n",
       "   'node_id': 'R_kgDOJ8YnuA',\n",
       "   'name': 'llama-cookbook',\n",
       "   'full_name': 'meta-llama/llama-cookbook',\n",
       "   'private': False,\n",
       "   'owner': {'login': 'meta-llama',\n",
       "    'id': 153379578,\n",
       "    'node_id': 'O_kgDOCSRi-g',\n",
       "    'avatar_url': 'https://avatars.githubusercontent.com/u/153379578?v=4',\n",
       "    'gravatar_id': '',\n",
       "    'url': 'https://api.github.com/users/meta-llama',\n",
       "    'html_url': 'https://github.com/meta-llama',\n",
       "    'followers_url': 'https://api.github.com/users/meta-llama/followers',\n",
       "    'following_url': 'https://api.github.com/users/meta-llama/following{/other_user}',\n",
       "    'gists_url': 'https://api.github.com/users/meta-llama/gists{/gist_id}',\n",
       "    'starred_url': 'https://api.github.com/users/meta-llama/starred{/owner}{/repo}',\n",
       "    'subscriptions_url': 'https://api.github.com/users/meta-llama/subscriptions',\n",
       "    'organizations_url': 'https://api.github.com/users/meta-llama/orgs',\n",
       "    'repos_url': 'https://api.github.com/users/meta-llama/repos',\n",
       "    'events_url': 'https://api.github.com/users/meta-llama/events{/privacy}',\n",
       "    'received_events_url': 'https://api.github.com/users/meta-llama/received_events',\n",
       "    'type': 'Organization',\n",
       "    'user_view_type': 'public',\n",
       "    'site_admin': False},\n",
       "   'html_url': 'https://github.com/meta-llama/llama-cookbook',\n",
       "   'description': 'Welcome to the Llama Cookbook! This is your go to guide for Building with Llama: Getting started with Inference, Fine-Tuning, RAG. We also show you how to solve end to end problems using Llama model family and using them on various provider services  ',\n",
       "   'fork': False,\n",
       "   'url': 'https://api.github.com/repos/meta-llama/llama-cookbook',\n",
       "   'forks_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/forks',\n",
       "   'keys_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/keys{/key_id}',\n",
       "   'collaborators_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/collaborators{/collaborator}',\n",
       "   'teams_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/teams',\n",
       "   'hooks_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/hooks',\n",
       "   'issue_events_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/issues/events{/number}',\n",
       "   'events_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/events',\n",
       "   'assignees_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/assignees{/user}',\n",
       "   'branches_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/branches{/branch}',\n",
       "   'tags_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/tags',\n",
       "   'blobs_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/git/blobs{/sha}',\n",
       "   'git_tags_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/git/tags{/sha}',\n",
       "   'git_refs_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/git/refs{/sha}',\n",
       "   'trees_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/git/trees{/sha}',\n",
       "   'statuses_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/statuses/{sha}',\n",
       "   'languages_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/languages',\n",
       "   'stargazers_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/stargazers',\n",
       "   'contributors_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/contributors',\n",
       "   'subscribers_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/subscribers',\n",
       "   'subscription_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/subscription',\n",
       "   'commits_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/commits{/sha}',\n",
       "   'git_commits_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/git/commits{/sha}',\n",
       "   'comments_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/comments{/number}',\n",
       "   'issue_comment_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/issues/comments{/number}',\n",
       "   'contents_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/contents/{+path}',\n",
       "   'compare_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/compare/{base}...{head}',\n",
       "   'merges_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/merges',\n",
       "   'archive_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/{archive_format}{/ref}',\n",
       "   'downloads_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/downloads',\n",
       "   'issues_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/issues{/number}',\n",
       "   'pulls_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/pulls{/number}',\n",
       "   'milestones_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/milestones{/number}',\n",
       "   'notifications_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/notifications{?since,all,participating}',\n",
       "   'labels_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/labels{/name}',\n",
       "   'releases_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/releases{/id}',\n",
       "   'deployments_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/deployments',\n",
       "   'created_at': '2023-07-17T07:33:48Z',\n",
       "   'updated_at': '2025-06-18T03:31:35Z',\n",
       "   'pushed_at': '2025-06-16T23:49:06Z',\n",
       "   'git_url': 'git://github.com/meta-llama/llama-cookbook.git',\n",
       "   'ssh_url': 'git@github.com:meta-llama/llama-cookbook.git',\n",
       "   'clone_url': 'https://github.com/meta-llama/llama-cookbook.git',\n",
       "   'svn_url': 'https://github.com/meta-llama/llama-cookbook',\n",
       "   'homepage': 'https://www.llama.com/',\n",
       "   'size': 232295,\n",
       "   'stargazers_count': 17492,\n",
       "   'watchers_count': 17492,\n",
       "   'language': 'Jupyter Notebook',\n",
       "   'has_issues': True,\n",
       "   'has_projects': True,\n",
       "   'has_downloads': True,\n",
       "   'has_wiki': False,\n",
       "   'has_pages': False,\n",
       "   'has_discussions': False,\n",
       "   'forks_count': 2519,\n",
       "   'mirror_url': None,\n",
       "   'archived': False,\n",
       "   'disabled': False,\n",
       "   'open_issues_count': 54,\n",
       "   'license': {'key': 'mit',\n",
       "    'name': 'MIT License',\n",
       "    'spdx_id': 'MIT',\n",
       "    'url': 'https://api.github.com/licenses/mit',\n",
       "    'node_id': 'MDc6TGljZW5zZTEz'},\n",
       "   'allow_forking': True,\n",
       "   'is_template': False,\n",
       "   'web_commit_signoff_required': False,\n",
       "   'topics': ['ai',\n",
       "    'finetuning',\n",
       "    'langchain',\n",
       "    'llama',\n",
       "    'llama2',\n",
       "    'llm',\n",
       "    'machine-learning',\n",
       "    'python',\n",
       "    'pytorch',\n",
       "    'vllm'],\n",
       "   'visibility': 'public',\n",
       "   'forks': 2519,\n",
       "   'open_issues': 54,\n",
       "   'watchers': 17492,\n",
       "   'default_branch': 'main'}},\n",
       " 'base': {'label': 'meta-llama:main',\n",
       "  'ref': 'main',\n",
       "  'sha': 'ddd7429c6191feec6a5b1c35d59b4d843c05b761',\n",
       "  'user': {'login': 'meta-llama',\n",
       "   'id': 153379578,\n",
       "   'node_id': 'O_kgDOCSRi-g',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/153379578?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/meta-llama',\n",
       "   'html_url': 'https://github.com/meta-llama',\n",
       "   'followers_url': 'https://api.github.com/users/meta-llama/followers',\n",
       "   'following_url': 'https://api.github.com/users/meta-llama/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/meta-llama/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/meta-llama/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/meta-llama/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/meta-llama/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/meta-llama/repos',\n",
       "   'events_url': 'https://api.github.com/users/meta-llama/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/meta-llama/received_events',\n",
       "   'type': 'Organization',\n",
       "   'user_view_type': 'public',\n",
       "   'site_admin': False},\n",
       "  'repo': {'id': 667297720,\n",
       "   'node_id': 'R_kgDOJ8YnuA',\n",
       "   'name': 'llama-cookbook',\n",
       "   'full_name': 'meta-llama/llama-cookbook',\n",
       "   'private': False,\n",
       "   'owner': {'login': 'meta-llama',\n",
       "    'id': 153379578,\n",
       "    'node_id': 'O_kgDOCSRi-g',\n",
       "    'avatar_url': 'https://avatars.githubusercontent.com/u/153379578?v=4',\n",
       "    'gravatar_id': '',\n",
       "    'url': 'https://api.github.com/users/meta-llama',\n",
       "    'html_url': 'https://github.com/meta-llama',\n",
       "    'followers_url': 'https://api.github.com/users/meta-llama/followers',\n",
       "    'following_url': 'https://api.github.com/users/meta-llama/following{/other_user}',\n",
       "    'gists_url': 'https://api.github.com/users/meta-llama/gists{/gist_id}',\n",
       "    'starred_url': 'https://api.github.com/users/meta-llama/starred{/owner}{/repo}',\n",
       "    'subscriptions_url': 'https://api.github.com/users/meta-llama/subscriptions',\n",
       "    'organizations_url': 'https://api.github.com/users/meta-llama/orgs',\n",
       "    'repos_url': 'https://api.github.com/users/meta-llama/repos',\n",
       "    'events_url': 'https://api.github.com/users/meta-llama/events{/privacy}',\n",
       "    'received_events_url': 'https://api.github.com/users/meta-llama/received_events',\n",
       "    'type': 'Organization',\n",
       "    'user_view_type': 'public',\n",
       "    'site_admin': False},\n",
       "   'html_url': 'https://github.com/meta-llama/llama-cookbook',\n",
       "   'description': 'Welcome to the Llama Cookbook! This is your go to guide for Building with Llama: Getting started with Inference, Fine-Tuning, RAG. We also show you how to solve end to end problems using Llama model family and using them on various provider services  ',\n",
       "   'fork': False,\n",
       "   'url': 'https://api.github.com/repos/meta-llama/llama-cookbook',\n",
       "   'forks_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/forks',\n",
       "   'keys_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/keys{/key_id}',\n",
       "   'collaborators_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/collaborators{/collaborator}',\n",
       "   'teams_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/teams',\n",
       "   'hooks_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/hooks',\n",
       "   'issue_events_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/issues/events{/number}',\n",
       "   'events_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/events',\n",
       "   'assignees_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/assignees{/user}',\n",
       "   'branches_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/branches{/branch}',\n",
       "   'tags_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/tags',\n",
       "   'blobs_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/git/blobs{/sha}',\n",
       "   'git_tags_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/git/tags{/sha}',\n",
       "   'git_refs_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/git/refs{/sha}',\n",
       "   'trees_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/git/trees{/sha}',\n",
       "   'statuses_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/statuses/{sha}',\n",
       "   'languages_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/languages',\n",
       "   'stargazers_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/stargazers',\n",
       "   'contributors_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/contributors',\n",
       "   'subscribers_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/subscribers',\n",
       "   'subscription_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/subscription',\n",
       "   'commits_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/commits{/sha}',\n",
       "   'git_commits_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/git/commits{/sha}',\n",
       "   'comments_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/comments{/number}',\n",
       "   'issue_comment_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/issues/comments{/number}',\n",
       "   'contents_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/contents/{+path}',\n",
       "   'compare_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/compare/{base}...{head}',\n",
       "   'merges_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/merges',\n",
       "   'archive_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/{archive_format}{/ref}',\n",
       "   'downloads_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/downloads',\n",
       "   'issues_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/issues{/number}',\n",
       "   'pulls_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/pulls{/number}',\n",
       "   'milestones_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/milestones{/number}',\n",
       "   'notifications_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/notifications{?since,all,participating}',\n",
       "   'labels_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/labels{/name}',\n",
       "   'releases_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/releases{/id}',\n",
       "   'deployments_url': 'https://api.github.com/repos/meta-llama/llama-cookbook/deployments',\n",
       "   'created_at': '2023-07-17T07:33:48Z',\n",
       "   'updated_at': '2025-06-18T03:31:35Z',\n",
       "   'pushed_at': '2025-06-16T23:49:06Z',\n",
       "   'git_url': 'git://github.com/meta-llama/llama-cookbook.git',\n",
       "   'ssh_url': 'git@github.com:meta-llama/llama-cookbook.git',\n",
       "   'clone_url': 'https://github.com/meta-llama/llama-cookbook.git',\n",
       "   'svn_url': 'https://github.com/meta-llama/llama-cookbook',\n",
       "   'homepage': 'https://www.llama.com/',\n",
       "   'size': 232295,\n",
       "   'stargazers_count': 17492,\n",
       "   'watchers_count': 17492,\n",
       "   'language': 'Jupyter Notebook',\n",
       "   'has_issues': True,\n",
       "   'has_projects': True,\n",
       "   'has_downloads': True,\n",
       "   'has_wiki': False,\n",
       "   'has_pages': False,\n",
       "   'has_discussions': False,\n",
       "   'forks_count': 2519,\n",
       "   'mirror_url': None,\n",
       "   'archived': False,\n",
       "   'disabled': False,\n",
       "   'open_issues_count': 54,\n",
       "   'license': {'key': 'mit',\n",
       "    'name': 'MIT License',\n",
       "    'spdx_id': 'MIT',\n",
       "    'url': 'https://api.github.com/licenses/mit',\n",
       "    'node_id': 'MDc6TGljZW5zZTEz'},\n",
       "   'allow_forking': True,\n",
       "   'is_template': False,\n",
       "   'web_commit_signoff_required': False,\n",
       "   'topics': ['ai',\n",
       "    'finetuning',\n",
       "    'langchain',\n",
       "    'llama',\n",
       "    'llama2',\n",
       "    'llm',\n",
       "    'machine-learning',\n",
       "    'python',\n",
       "    'pytorch',\n",
       "    'vllm'],\n",
       "   'visibility': 'public',\n",
       "   'forks': 2519,\n",
       "   'open_issues': 54,\n",
       "   'watchers': 17492,\n",
       "   'default_branch': 'main'}},\n",
       " '_links': {'self': {'href': 'https://api.github.com/repos/meta-llama/llama-cookbook/pulls/962'},\n",
       "  'html': {'href': 'https://github.com/meta-llama/llama-cookbook/pull/962'},\n",
       "  'issue': {'href': 'https://api.github.com/repos/meta-llama/llama-cookbook/issues/962'},\n",
       "  'comments': {'href': 'https://api.github.com/repos/meta-llama/llama-cookbook/issues/962/comments'},\n",
       "  'review_comments': {'href': 'https://api.github.com/repos/meta-llama/llama-cookbook/pulls/962/comments'},\n",
       "  'review_comment': {'href': 'https://api.github.com/repos/meta-llama/llama-cookbook/pulls/comments{/number}'},\n",
       "  'commits': {'href': 'https://api.github.com/repos/meta-llama/llama-cookbook/pulls/962/commits'},\n",
       "  'statuses': {'href': 'https://api.github.com/repos/meta-llama/llama-cookbook/statuses/6477af074bb1724bd42d14831f341d7525e63b4c'}},\n",
       " 'author_association': 'CONTRIBUTOR',\n",
       " 'auto_merge': None,\n",
       " 'active_lock_reason': None}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pull_requests[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ec81999-eb77-48aa-be91-72cc2a1f1813",
   "metadata": {
    "height": 215
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import get_pr_content\n",
    "\n",
    "all_content = []\n",
    "for pr in pull_requests:\n",
    "    pr_number = pr['number']\n",
    "    content = get_pr_content(repo_owner, repo_name,\n",
    "                             pr_number, github_access_token) # a github access token is needed to avoid the lower rate limit of making github requeests\n",
    "    if content:\n",
    "        all_content.append(f\"PR #{pr_number}: {content}\")\n",
    "\n",
    "len(all_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3266383-69ad-4240-a0a0-1be2c627c84c",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1051214, 260468)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pr_content = \"\\n\\n\".join(all_content)\n",
    "len(all_pr_content), len(tokenizer.encode(all_pr_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ac2452f-946e-435e-8929-5f82549c0b3a",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine how many PRs are about Android or iOS, let's examine the provided PR information and identify relevant PRs.\n",
      "\n",
      "The following PRs mention Android or iOS:\n",
      "\n",
      "1. **PR #962**: No mention of Android or iOS.\n",
      "2. **PR #959**: No mention of Android or iOS.\n",
      "3. **PR #958**: No mention of Android or iOS.\n",
      "4. **PR #957**: No mention of Android or iOS.\n",
      "5. **PR #956**: No mention of Android or iOS.\n",
      "6. **PR #954**: No mention of Android or iOS.\n",
      "7. **PR #953**: No mention of Android or iOS.\n",
      "8. **PR #951**: No mention of Android or iOS.\n",
      "9. **PR #950**: No mention of Android or iOS.\n",
      "10. **PR #944**: No mention of Android or iOS.\n",
      "11. **PR #943**: No mention of Android or iOS.\n",
      "12. **PR #942**: No mention of Android or iOS.\n",
      "13. **PR #941**: No mention of Android or iOS.\n",
      "14. **PR #940**: No mention of Android or iOS.\n",
      "15. **PR #939**: No mention of Android or iOS.\n",
      "16. **PR #937**: No mention of Android or iOS.\n",
      "17. **PR #935**: **PR #935: [Hotfix] Pytest Workflow AndroidManifest.xml issue** - This PR is about Android.\n",
      "18. **PR #932**: No mention of Android or iOS.\n",
      "19. **PR #926**: **PR #926: Update android app** - This PR is about Android.\n",
      "20. **PR #915**: **PR #915: Add Android Article Summarizer demo app** - This PR is about Android.\n",
      "\n",
      "Based on the provided information, there are **3 PRs** related to Android or iOS:\n",
      "\n",
      "*   PR #935: [Hotfix] Pytest Workflow AndroidManifest.xml issue\n",
      "*   PR #926: Update android app\n",
      "*   PR #915: Add Android Article Summarizer demo app\n",
      "\n",
      "There are no PRs explicitly mentioning iOS. Therefore, out of the given PRs, **3** are related to Android, and **0** are related to iOS.\n"
     ]
    }
   ],
   "source": [
    "query = \"how many PRs are about android or iOS?\"\n",
    "print(llama4_together(f\"\"\"{query}\n",
    "Below is all the PR info:\n",
    "{all_pr_content}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1f79641-d850-4814-a98f-e08fdc1731ac",
   "metadata": {
    "height": 132
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 PRs about agents:\n",
      "\n",
      "1. **PR #825: Llama code/code review agents**: This PR showcases how to build a code review agent using Llama 3.1 models, specifically leveraging its function calling capabilities to review code changes and provide feedback.\n",
      "\n",
      "2. **PR #706: Adds Llama 3.2 example on Modal with a fun experiment**: This PR introduces a serverless Python platform called Modal, where users can run experiments on Llama models.\n",
      "\n",
      "3. **PR #627: Eval reproduce recipe using lm-evaluation-harness and our 3.1 evals datasets**: This PR provides a guide on reproducing Meta Llama 3.1 evaluation metrics using lm-evaluation-harness and Meta Llama 3.1 evals datasets.\n",
      "\n",
      "4. **PR #618: [Recipe] Example featuring built-in tool calling capabilities - Wolfram Alpha, Interpreter, Brave Search**: This PR introduces examples powered by OctoAI that showcase Llama 3.1's tool calling capabilities.\n",
      "\n",
      "5. **PR #597: Fix broken image link**: This PR fixes a broken image link in the README file.\n",
      "\n",
      "6. **PR #825: Llama code/code review agents**: This PR introduces a code review agent using Llama 3.1 models.\n",
      "\n",
      "Summary of agent PRs: These PRs focus on introducing new examples and recipes for using Llama models as agents in various applications, such as code review, data analysis, and automation tasks. They also update the documentation and fix broken links. Overall, these PRs aim to demonstrate the capabilities of Llama models in agentic implementations and provide users with practical examples and guides for utilizing these models in their own projects.\n"
     ]
    }
   ],
   "source": [
    "query = f\"\"\"how many PRs are about agents? give a one-sentence\n",
    "summary of each, then a summary of all those agent PRs.\"\"\"\n",
    "\n",
    "print(llama4_together(f\"\"\"{query}\n",
    "Below is all the PR info:\n",
    "{all_pr_content}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e283ea3-77a0-4120-9b99-e63dfbf388fa",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546162af-e8a7-4f37-9c30-4d8a071941e4",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d8d953-cd71-4f4a-a115-6dbd5b318a43",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7016b5-3835-40bd-b0a4-9484318abe25",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee7318e-6585-4544-b4d4-b2bbd53e93ca",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62631094-ad78-41a2-a2d5-1998cd82f69b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
